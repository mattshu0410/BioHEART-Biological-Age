---
title: "BioHEART"
author: "Matthew Shu"
date: "16/08/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(dplyr)
library(glmnet)
library(caret)
library(corrplot)
library(naniar)
library(factoextra)
#library(sigFeature)
library(blockForest)
#source("kdm_calc.R")

# Modeling packages
library(tensorflow)
library(keras)         # for fitting DNNs
library(tfruns)        # for additional grid search & model training functions

library(boot)
library(CVrisk)
library(limma)
library(plotly)
```

# Biological Age

## Data Exploration

```{r}
load("MultiAssayExperiment_20220816.RData")
load("FRS_data.RData")
```

```{r}
# SummarizedExperiment
experiments(bioheart_mae)$Metabolomics

# Variable Definitions
metadata(bioheart_mae)$variable_definition

# Patient Clinical Data
colData(bioheart_mae)

dat_FRS

# FRS
dat_FRS %>%
  filter(ethcat == 5) %>%
  select(age) %>%
  pull() %>%
  hist()
```

```{r}
data.frame(metadata(bioheart_mae)$variable_definition)

metadata(colData(bioheart_mae))

data.frame(colData(bioheart_mae))
table(bioheart_mae$gender)

upsetSamples(bioheart_mae)
```

```{r}
se_cytof = bioheart_mae[['CyTOF']]
cytof_exp = assays(se_cytof)$CyTOF
cytof_exp
toString(row.names(data.frame(assays(se_cytof)$CyTOF)))
```

```{r}
se_metab = bioheart_mae[['Metabolomics']]
data.frame(assays(se_metab)$log2Raw)
assays(se_metab)
toString(row.names(data.frame(assays(se_metab)$log2Raw)))
```

```{r}
se_lipid_spec = bioheart_mae[['Lipidomics_species']]
data.frame(assays(se_lipid_spec)$log2conc)
```

```{r}
se_lipid_tot = bioheart_mae[['Lipidomics_totals']]
data.frame(assays(se_lipid_tot)$log2totalsConc)


toString(row.names(data.frame(assays(se_lipid_tot)$log2totalsConc)))
```

## Functions

```{r}


# Takes a X_test, y_test, model and n (bootstrap samples)
bootstrap_metric = function(X, y, model, n) {
  metric = data.frame(RMSE=double(), R2 = double(), MAE = double())
  print(metric)
  for (i in c(1:n)) {
    indices = sample(nrow(X), size=nrow(X), replace=TRUE)
    bs_X = X[indices,]
    bs_y = y[indices]
    pred = predict(model, bs_X)
    
    metric = rbind(metric, 
                   cbind(RMSE = RMSE(pred, bs_y), 
                         R2 = R2(pred, bs_y), 
                         MAE = MAE(pred, bs_y)))
    
  }
  
  pred = predict(model, X)
  a = metric
  print(a)
  print(c(LB = mean(a$R2)-1.96*sd(a$R2), UB = mean(a$R2)+1.96*sd(a$R2)))
  df = cbind(
        rbind(RMSE = RMSE(pred, y), R2 = R2(pred, y), MAE = MAE(pred, y)),
        rbind(
          c(LB = mean(a$RMSE)-1.96*sd(a$RMSE), UB = mean(a$RMSE)+1.96*sd(a$RMSE)),
          c(LB = mean(a$R2)-1.96*sd(a$R2), UB = mean(a$R2)+1.96*sd(a$R2)),
          c(LB = mean(a$MAE)-1.96*sd(a$MAE), UB = mean(a$MAE)+1.96*sd(a$MAE))
        )
      )
  df = round(df, 2)
  return(
    df
  )
}


bootstrap_metric(X_test, y_test, elastic_model, 200)




```

## Proteomic Age

### Cleaning

```{r}
se_prot = bioheart_mae[['Proteomics']]
prot_exp = data.frame(assays(se_prot)$raw)
toString(row.names(data.frame(assays(se_prot)$raw)))
```

```{r}
exp = longFormat(bioheart_mae[,,'Proteomics'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"))
exp 


unique(exp$rowname)
```

Get a clean dataframe

```{r}

# DATASET w/ FRS
df = data.frame(exp) %>%
  filter(!grepl("Repeat", colname)) %>%
  filter(primary != "79") %>%
  filter(primary != "Pool") %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(primary = as.numeric(primary), cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini)) %>%
  left_join(dat_FRS %>%
              select(record_id, cvd_FRS, chd_FRS),
            by = c("primary"="record_id"))

# DATASET w/ NO FRS
df = data.frame(exp) %>%
  filter(!grepl("Repeat", colname)) %>%
  filter(primary != "79") %>%
  filter(primary != "Pool") %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini))

# Check duplicates
#data.frame(exp) %>%
#  filter(!grepl("Repeat", colname)) %>%
#  filter(primary != "79") %>%
#  filter(primary != "Pool") %>%
#  group_by(primary, age, rowname) %>%
#  summarise(n = n(), .groups = "drop") %>%
#  filter(n > 1L)

```

### Simple Linear Model

```{r}

# X and Y datasets
y = df %>% 
    select(age) %>% 
    #scale(center = TRUE, scale = FALSE) %>% 
    as.matrix()

X = df %>% 
    select(-age, -primary, -smurfs, -cacs, -cacs_pct, -gensini) %>% 
    as.matrix()

# Linear Model

slm = lm(y~ X)
summary(slm)


```

### Correlation Plot

Anything above correlation 90 is detected.

```{r}



corr_simple <- function(data=df,sig=0.5){
  #convert data to numeric in order to run correlations
  #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
  df_cor <- data %>% mutate_if(is.character, as.factor)
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}

histogram(c(cor(X)))
corr_simple(data.frame(X), sig=0.85)

```

### Elastic Model

-   Clearly there is correlation between metabolites so we need to penalise complexity
-   Going from 1000 to 260 metabolites

R\^2 = 0.4896232

```{r}

# Train Test Split

set.seed(2022)
p = 0.7 
train = sample(1:nrow(X), nrow(X)*p)


# Model Building : Elastic Net Regression
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verboseIter = TRUE)

X_train = X[train,]
y_train = y[train,]
X_test = X[-train,]
y_test = y[-train,]

# Training Elastic Net Regression model
elastic_model = caret::train(age ~ .,
            data = cbind(X[train,], "age" = y[train,]),
            method = "glmnet",
            tuneLength = 25,
            trControl = control)

data.frame(as.matrix(coef(elastic_model$finalModel, elastic_model$bestTune$lambda))) %>%
  filter(s1 != 0)



elastic_model
```

### Testing Data Performance

-   0.56 isn't amazing

```{r}

# Testing on Data Left-Out
elastic_model$bestTune
X_test = X[-train,]
y_test = y[-train,]

metrics = bootstrap_metric(X_test, y_test, elastic_model, 200)
paste("(",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste("(",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")
```

### Using PCA

```{r}
pca_df = X[train,]
pca = prcomp(pca_df)
summary(pca)

screeplot(pca, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

cumpro = cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 5, col="blue", lty=5)
# abline(h = 0.63305, col="blue", lty=5)
abline(h = 0.9, col="red", lty=5)
legend("topleft", legend=c("Cut-off @ PC5", "90% Explained Variance"),
       col=c("blue", "red"), lty=5, cex=0.6)


```

```{r}
# PCA Axes for Subjects
axes = predict(pca, X[train,])
pca_dat = cbind(X[train,], axes, age = y[train,])

PCA_model = lm(age ~ PC1 + PC2 + PC3 + PC4, data.frame(pca_dat))
summary(PCA_model)


# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]
axes = predict(pca, X_test)

pred = predict(PCA_model, data.frame(axes))


data.frame(
  RMSE = RMSE(pred, y_test),
  Rsquare = R2(pred, y_test)
)

metrics = bootstrap_metric(data.frame(axes), y_test, PCA_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")
```

### Using Random Forest

Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to "large p, small n" problems, and is able to account for correlation as well as interactions among features.

<https://www.sciencedirect.com/science/article/pii/S0888754312000626>

```{r}
ntree = 500

# Model Building : Random Forest
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verbose = TRUE)

rf_model =  caret::train(age ~ .,
                         data = cbind(X[train,], "age" = y[train,]),
                         method = 'rf',
                         tuneLength  = 15, 
                         trControl = control)

```

```{r}

# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]

pred = predict(rf_model, X_test)

metrics = bootstrap_metric(X_test, y_test, rf_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Using XGBoost

```{r}
library(xgboost)
xgb_train = xgb.DMatrix(data = X_train, label = y_train)
xgb_test = xgb.DMatrix(data = X_test, label = y_test)

# Watchlist
watchlist = list(train=xgb_train, test=xgb_test)


#fit XGBoost model
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)

#define final model
model_xgboost = xgboost(data = xgb_train, max.depth = 3, nrounds = 65, verbose = 0)
summary(model_xgboost)
pred = predict(model_xgboost, xgb_test)

boxplot(abs(pred-y_test))

data.frame(
  RMSE = RMSE(pred, y_test),
  Rsquare = R2(pred, y_test)
)

```

### Autoencoder

```{r}
library(h2o)
h2o.init()

```

```{r}


features = as.h2o(X_train)

# Training Autoencoder on Features

ae1 = h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = c(500,275,500),
  activation = "Tanh"
)

# Pulling Deep Features
ae1_codings = h2o.deepfeatures(ae1, features, layer = 2)
X_codings = as.data.frame(ae1_codings)

# Regression on Deep Features
slm = lm(y_train ~ ., cbind(y_train, X_codings))
summary(slm)

# Model Predict
test_features = as.h2o(X_test)
test_features = h2o.na_omit(test_features)
test_codings = h2o.deepfeatures(ae1, test_features, layer = 2)
test_codings_df = as.data.frame(test_codings)
pred = predict(slm, test_codings_df)
boxplot(abs(pred-y_test))


metrics = bootstrap_metric(test_codings_df, y_test, slm, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")



```

My attempt at trying to see if there is a 2D projection of the deep features that shows any pattern.

```{r}

ae2 = h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = 2,
  activation = "Tanh"
)

ae2_codings = h2o.deepfeatures(ae2, features, layer = 1)



cbind(
  df[train,],
  as.data.frame(ae2_codings)
  ) %>%
  ggplot() +
  aes(x = `DF.L1.C1`, y = `DF.L1.C2`, color = cacs_pct) +
  geom_point()

```

### Support Vector Regression

```{r}
library(e1071)
library(kernlab)

model = caret::train(
  y_train ~ .,
  data = cbind(y_train, X_train),
  method = 'svmRadial'
)
model

```

### Graphing Exploration

Setting up dataframe

```{r}

age_accl = pred-y_test

graph_df = cbind(df[-train,],y_test, pred, age_accl) %>%
  select(primary, age, smurfs, cacs, cacs_pct, gensini, y_test, pred, age_accl)

```

Proteomic Age vs Chronological Age

```{r}
mean = mean(graph_df$smurfs)

graph_df %>%
  filter(is.na(smurfs) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = pred) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Proteomic Age"
  ) +
  theme_bw()
```

Age Acceleration

```{r}

ggplot(graph_df) +
  aes(x = age_accl) +
  geom_histogram(bins = 100) +
  theme_bw() +
  labs(
    x = "Proteomic Age Acceleration",
    y = "Frequency"
  )

graph_df = graph_df %>%
  filter(is.na(cacs_pct) == FALSE)

mid = mean(graph_df$cacs_pct)

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = cacs_pct) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Age Acceleration",
    title = "CACS for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()
```

```{r}

graph_df = graph_df %>%
  filter(is.na(gensini) == FALSE)

mid = mean(graph_df$gensini)

graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = gensini) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Age Acceleration",
    title = "Gensini for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()
```

## Metabolomics Age

hRUV - Hierarchical Approach to Removal of Unwanted Variation

<https://www.biorxiv.org/content/10.1101/2020.12.21.423723v1.full.pdf>

`SummarizedExperiment` implements matrix-like behaviour

How to use `wideFormat` and `longFormat` for `SummarizedExperiment` w/ multiple assays and picking a very specific assay

`i` is the i-th assay in the `SummarizedExperiment` objects; this provides vector input in the case the the object has more than one assay

<https://support.bioconductor.org/p/110228/>

Cleaning

-   Removed data from pooled samples
-   1197 rows before removal of rows without cacs, 1195 rows after remove
-   503 NA by coercion

```{r}


# Picks the first assay rlmSampleAllShort_H_batch
exp = longFormat(bioheart_mae[,,'Metabolomics'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 3L)

df = data.frame(exp) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  filter(if_all(everything(), ~ .!=".")) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini)) %>%
  drop_na() %>%
  mutate(across(8:60, ~(.-min(.))/((max(.)-min(.)))))

boxplot(df[,8:60])


```

### Elastic Model

Elastic model reduces 60 metabolites to 42 metabolites in the Elastic Net model. I think my main concern is that this is targeted.

R\^2=0.13 RMSE 11.8164

```{r}

# X and Y datasets
y = df %>% 
    select(age) %>% 
    #scale(center = TRUE, scale = FALSE) %>% 
    as.matrix()

X = df %>% 
    select(-age, -primary, -smurfs, -cacs, -cacs_pct, -gensini, -colname) %>% 
    as.matrix()


# Train Test Split


p = 0.7 
train = sample(1:nrow(X), nrow(X)*p)

X_train = X[train,]
y_train = y[train,]
X_test = X[-train,]
y_test = y[-train,]

# Model Building : Elastic Net Regression
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verboseIter = TRUE)


# Training Elastic Net Regression model
elastic_model = caret::train(age ~ .,
                             data = cbind(X[train,], "age" = y[train,]),
                             method = "glmnet",
                             tuneLength = 25,
                             trControl = control)

data.frame(as.matrix(coef(elastic_model$finalModel, elastic_model$bestTune$lambda))) %>%
  filter(s1 != 0)

elastic_model

# Testing on Data Left-Out
elastic_model$bestTune
X_test = X[-train,]
y_test = y[-train,]

pred = predict(elastic_model, X_test)

metrics = bootstrap_metric(X_test, y_test, elastic_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Using PCA and KDM

R\^2 = 0.06061582 RMSE = 12.37645

<https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff> <https://online.stat.psu.edu/stat505/lesson/11/11.4#>:\~:text=We%20use%20the%20correlations%20between,square%20root%20of%20the%20eigenvalue. <https://stats.stackexchange.com/questions/133149/why-is-variance-instead-of-standard-deviation-the-default-measure-of-informati>

```{r}
pca_df = X[train,]
pca = prcomp(pca_df)
summary(pca)

screeplot(pca, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

cumpro = cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 5, col="blue", lty=5)
# abline(h = 0.63305, col="blue", lty=5)
abline(h = 0.9, col="red", lty=5)
legend("topleft", legend=c("Cut-off @ PC5", "90% Explained Variance"),
       col=c("blue", "red"), lty=5, cex=0.6)


```

-   KDM - uses chronological age, BM are a function of BA and CA.

<https://academic.oup.com/biomedgerontology/article/74/Supplement_1/S52/5625183>

```{r}
# PCA Axes for Subjects
axes = predict(pca, X[train,])
pca_dat = cbind(X[train,], axes, age = y[train,])

PCA_model = lm(age ~ PC1 + PC2 + PC3 + PC4 + PC5, data.frame(pca_dat))
summary(PCA_model)


# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]
axes = predict(pca, X_test)

pred = predict(PCA_model, data.frame(axes))

metrics = bootstrap_metric(data.frame(axes), y_test,PCA_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")
```

```{r}
kdm_calc(
  pca,
)

```

### Using Random Forest

Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to "large p, small n" problems, and is able to account for correlation as well as interactions among features.

RMSE = 11.29 R\^2 = 0.22

<https://www.sciencedirect.com/science/article/pii/S0888754312000626>

```{r}
ntree = 500

# Model Building : Random Forest
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verbose = TRUE)

rf_model =  caret::train(age ~ .,
                         data = cbind(X[train,], "age" = y[train,]),
                         method = 'rf',
                         tuneLength  = 15, 
                         trControl = control)

```

```{r}

# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]

pred = predict(rf_model, X_test)

metrics = bootstrap_metric(X_test, y_test, rf_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

```{r}
plot(rf_model)

```

### Random Block Forest

OOB prediction error (MSE): 123.0806 R squared (OOB): 0.1451966

```{r}

# Not as useful here since this is only a single block so I expect similar results to the random forest
blockforobj = blockfor(X[train,], y[train,], blocks = list(seq(1:ncol(X[train,]))), num.trees = 100)
blockforobj
```

### Using Neural Network

iAge

2 fully connected layers with 5 nodes in each layer and tanh activation

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(256, activation = 'relu') %>%
  layer_dense(64, activation = 'relu') %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam(0.001)
)

```

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4931851/>

I copied an architecture to see if it did better. Pretty BAD.

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(500, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(250, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(100, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(50, activation = 'PReLU',kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adagrad(0.001)
)

```

L1 Regularisation + Batch Norm

```{r}
model = keras_model_sequential() %>%
  
  # Network architecture with L1 regularization and batch normalization
  layer_dense(units = 256, activation = "relu",
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 128, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 64, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 1) %>%

  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

Model with Dropout

```{r}

model = keras_model_sequential() %>%
  
  # Network architecture with 20% dropout
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1) %>%
  
  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4931851/#> Best model from this paper

```{r}
model = keras_model_sequential() %>%
  
  # Network architecture with 20% dropout
  layer_dense(units = 2000, activation = 'PReLU', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1500, activation = 'PReLU', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 1000, activation = 'PReLU', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 500, activation = 'PReLU', kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_dense(units = 1) %>%
  
  # Backpropagation
  compile(
    loss = 'mean_squared_error',
    optimizer = optimizer_adagrad(0.005)
  )

```

```{r}

# Train Model
fit1 = model %>%
  fit(
    x = X_train,
    y = y_train,
    epochs = 100,
    batch_size = 128,
    validation_split = 0.2,
    verbose = TRUE
  )

# Model Predict
pred = predict(model, X_test)

boxplot(abs(pred-y_test))

metrics = bootstrap_metric(X_test, y_test,model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

```{r}
reticulate::source_python("M:/Github-Version-Control-Projects/GAE/gae/model/trainer.py")
gae = reticulate::import_from_path("gae", path = "M:/Github-Version-Control-Projects/GAE/")
reticulate::virtualenv_install("r-reticulate", "theano")
Sys.setenv(RETICULATE_PYTHON = "C:/Program Files/Python38/python.exe")
library(reticulate)
reticulate::use_condaenv(condaenv="r-reticulate", required = TRUE)
py_config()
use_python("C:/Program Files/Python38/python.exe")
```

### Using XGBoost

```{r}
library(xgboost)
xgb_train = xgb.DMatrix(data = X_train, label = y_train)
xgb_test = xgb.DMatrix(data = X_test, label = y_test)

# Watchlist
watchlist = list(train=xgb_train, test=xgb_test)


#fit XGBoost model
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)

#define final model
model_xgboost = xgboost(data = xgb_train, max.depth = 3, nrounds = 65, verbose = 0)
summary(model_xgboost)
pred = predict(model_xgboost, xgb_test)

boxplot(abs(pred-y_test))

data.frame(
  RMSE = RMSE(pred, y_test),
  Rsquare = R2(pred, y_test)
)

```

### Autoencoder
```{r}
library(h2o)
h2o.init()

features = as.h2o(X_train)
# Strange bug where special characters cause extra row created
features = h2o.na_omit(features)

# Training Autoencoder on Features

ae1 = h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = 45,
  activation = "Tanh"
)

# Pulling Deep Features
ae1_codings = h2o.deepfeatures(ae1, features, layer = 1)
X_codings = as.data.frame(ae1_codings)

# Regression on Deep Features
slm = lm(y_train ~ ., cbind(y_train, X_codings))
summary(slm)

# Model Predict
test_features = as.h2o(X_test)
test_features = h2o.na_omit(test_features)
test_codings = h2o.deepfeatures(ae1, test_features, layer = 1)
test_codings_df = as.data.frame(test_codings)
pred = predict(slm, test_codings_df)
boxplot(abs(pred-y_test))


metrics = bootstrap_metric(test_codings_df, y_test, slm, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Graphs

```{r}




# Calculating Age Acceleration

age_accl = pred-y_test

graph_df = cbind(df[-train,],y_test, pred, age_accl) %>%
  select(primary, age, smurfs, cacs, cacs_pct, gensini, y_test, pred, age_accl)

# Graph of Metabolomic Age vs Chronological Age

mean = mean(graph_df$smurfs)

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = pred, color = cacs_pct) +
  geom_point() +
  geom_abline(a=0, b=1) +
  labs(
    x = "Chronological Age",
    y = "Metabolomic Age"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()

# Age Acceleration Histogram

ggplot(graph_df) +
  aes(x = age_accl) +
  geom_histogram(bins = 100) +
  theme_bw() +
  labs(
    x = "Metabolomic Age Acceleration",
    y = "Frequency"
  )

# CACS vs Age Acceleration


mid = graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  summarise(mean = mean(cacs_pct)) %>%
  pull()

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = cacs_pct) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Metabolomic Age Acceleration",
    title = "CACS for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()

# SMURFS vs Age Acceleration

mid = mean(graph_df$smurfs)

graph_df %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = smurfs) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Metabolomic Age Acceleration",
    title = "SMURFs for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()


# Gensini vs Age Acceleration

mid = graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  summarise(mean = mean(gensini)) %>%
  pull()


graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = gensini) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Metabolomic Age Acceleration",
    title = "Gensini for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()


```

## Lipidomics Age

Interestingly, lipidomic species gives a better R\^2

-   46 variables used by the Elastic Net Model when using Lipidomic Totals, R\^2 = 0.29
-   508ish variables used, R\^2 = .56, RMSE = 8.34

```{r}


# Picks the first assay rlmSampleAllShort_H_batch
exp = longFormat(bioheart_mae[,,'Lipidomics_species'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 2L)

exp = longFormat(bioheart_mae[,,'Lipidomics_totals'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 2L)


df = data.frame(exp) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini))



bioheart_mae[['Lipidomics_species']]

```

### Elastic Regression Approach

43 variables from 48 totals
429 variables from 832 species

```{r}
# Parallel Computing
library(doParallel)
cl = makePSOCKcluster(5)
registerDoParallel(cl)

# X and Y datasets
y = df %>% 
    select(age) %>% 
    #scale(center = TRUE, scale = FALSE) %>% 
    as.matrix()

X = df %>% 
    select(-age, -primary, -smurfs, -cacs, -cacs_pct, -gensini, -colname) %>% 
    as.matrix()


# Train Test Split

p = 0.7
train = sample(1:nrow(X), nrow(X)*p)

X_train = X[train,]
y_train = y[train,]
X_test = X[-train,]
y_test = y[-train,]


# Model Building : Elastic Net Regression
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verboseIter = TRUE)


# Training Elastic Net Regression model
elastic_model = caret::train(age ~ .,
                      data = cbind(X[train,], "age" = y[train,]),
                      method = "glmnet",
                      tuneLength = 25,
                      trControl = control)

# Number of Coefficients
coef_matrix = coef(elastic_model$finalModel, elastic_model$bestTune$lambda)

data.frame(as.matrix(coef(elastic_model$finalModel, elastic_model$bestTune$lambda))) %>%
  filter(s1 != 0)

# Testing on Data Left-Out
elastic_model$bestTune
X_test = X[-train,]
y_test = y[-train,]

pred = predict(elastic_model, X_test)

boxplot(abs(pred-y_test))


metrics = bootstrap_metric(X_test, y_test,elastic_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

## When you are done:
stopCluster(cl)

```

### Using PCA and KDM

The first 5 principal components explain just over 60% of the variance. The standard deviation is the square root of the eigenvalue; variances of PCs are eigenvalues of the covariance matrix.

For Lipidomic Species R\^2 = 0.226007 RMSE = 11.10364

<https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff> <https://online.stat.psu.edu/stat505/lesson/11/11.4#>:\~:text=We%20use%20the%20correlations%20between,square%20root%20of%20the%20eigenvalue. <https://stats.stackexchange.com/questions/133149/why-is-variance-instead-of-standard-deviation-the-default-measure-of-informati>

```{r}
pca_df = X[train,]
pca = prcomp(pca_df)
summary(pca)

screeplot(pca, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

cumpro = cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 5, col="blue", lty=5)
# abline(h = 0.63305, col="blue", lty=5)
abline(h = 0.9, col="red", lty=5)
legend("topleft", legend=c("Cut-off @ PC5", "90% Explained Variance"),
       col=c("blue", "red"), lty=5, cex=0.6)


```

```{r}
# PCA Axes for Subjects
axes = predict(pca, X[train,])
pca_dat = cbind(X[train,], axes, age = y[train,])

PCA_model = lm(age ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8, data.frame(pca_dat))
summary(PCA_model)


# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]
axes = predict(pca, X_test)

pred = predict(PCA_model, data.frame(axes))



metrics = bootstrap_metric(data.frame(axes), y_test,PCA_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

-   KDM - uses chronological age, BM are a function of BA and CA.

<https://academic.oup.com/biomedgerontology/article/74/Supplement_1/S52/5625183>

```{r}
kdm_calc(
  pca,
)

```

### Using Random Forest

Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to "large p, small n" problems, and is able to account for correlation as well as interactions among features.

RMSE = 11.96586 R\^2 = 0.1051555

<https://www.sciencedirect.com/science/article/pii/S0888754312000626>

```{r}
ntree = 500

# Model Building : Random Forest
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verbose = TRUE)

rf_model =  caret::train(age ~ .,
                  data = cbind(X[train,], "age" = y[train,]),
                  method = 'rf',
                  tuneLength  = 15, 
                  trControl = control)

```

```{r}

# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]

pred = predict(rf_model, X_test)

metrics = bootstrap_metric(X_test, y_test, rf_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Using Random BlockForest

-   Block selection randomization procedure (select subset of all M blocks with equal probability)
-   For each split, we sample fixed numbers of variables from each block separately. More precisely, for m=1,...,M we sample sqrt(p_m) variables from block m.

<https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2942-y#>

OOB prediction error (MSE): 127.0553 R squared (OOB): 0.1061645

```{r}

# Not as useful here since this is only a single block so I expect similar results to the random forest
blockforobj = blockfor(X[train,], y[train,], blocks = list(seq(1:ncol(X[train,]))), num.trees = 100)

```

```{r}

blockforobj
```

### Using XGBoost

```{r}
library(xgboost)
xgb_train = xgb.DMatrix(data = X_train, label = y_train)
xgb_test = xgb.DMatrix(data = X_test, label = y_test)

# Watchlist
watchlist = list(train=xgb_train, test=xgb_test)


#fit XGBoost model
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)

#define final model
model_xgboost = xgboost(data = xgb_train, max.depth = 3, nrounds = 65, verbose = 0)
summary(model_xgboost)
pred = predict(model_xgboost, xgb_test)

boxplot(abs(pred-y_test))

data.frame(
  RMSE = RMSE(pred, y_test),
  Rsquare = R2(pred, y_test)
)

```

### Using Neural Network

iAge

2 fully connected layers with 5 nodes in each layer and tanh activation

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(256, activation = 'relu') %>%
  layer_dense(64, activation = 'relu') %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam(0.001)
)

```

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4931851/>

I copied an architecture to see if it did better. Pretty BAD.

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(500, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(250, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(100, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(50, activation = 'PReLU',kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adagrad(0.001)
)

```

L1 Regularisation + Batch Norm

```{r}
model = keras_model_sequential() %>%
  
  # Network architecture with L1 regularization and batch normalization
  layer_dense(units = 256, activation = "relu",
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 128, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 64, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 1) %>%

  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

Model with Dropout

Model with Dropout

```{r}

model = keras_model_sequential() %>%
  
  # Network architecture with 20% dropout
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1) %>%
  
  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

```{r}

# Train Model
fit1 = model %>%
  fit(
    x = X_train,
    y = y_train,
    epochs = 100,
    batch_size = 128,
    validation_split = 0.2,
    verbose = TRUE
  )

# Model Predict
pred = predict(model, X_test)

boxplot(abs(pred-y_test))


metrics = bootstrap_metric(X_test, y_test,model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Autoencoder

```{r}
library(h2o)
h2o.init()

features = as.h2o(X_train)

# Training Autoencoder on Features

ae1 = h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = 290,
  activation = "Tanh"
)

# Pulling Deep Features
ae1_codings = h2o.deepfeatures(ae1, features, layer = 1)
X_codings = as.data.frame(ae1_codings)

# Regression on Deep Features
slm = lm(y_train ~ ., cbind(y_train, X_codings))
summary(slm)

# Model Predict
test_features = as.h2o(X_test)
test_features = h2o.na_omit(test_features)
test_codings = h2o.deepfeatures(ae1, test_features, layer = 1)
test_codings_df = as.data.frame(test_codings)
pred = predict(slm, test_codings_df)
boxplot(abs(pred-y_test))


metrics = bootstrap_metric(test_codings_df, y_test, slm, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Graphs

```{r}



# Calculating Age Acceleration
# negative age_accl means younger than chronological
# positive age_accl means older than chronological

age_accl = pred-y_test
age_accl_adj = residuals(lm(age_accl ~ y_test))

graph_df = cbind(df[-train,],y_test, pred, age_accl, age_accl_adj) %>%
select(primary, age, smurfs, cacs, cacs_pct, gensini, y_test, pred, age_accl, age_accl_adj) %>%
mutate(cacsplus = case_when(
  cacs_pct > 75 ~ "actionable",
  cacs > 100 ~ "actionable",
  TRUE ~ "non-actionable"
))


# Graph of Lipidomic Age vs Chronological Age


mid = graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  summarise(mean = mean(cacs_pct)) %>%
  pull()

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() + 
  geom_abline(intercept = 0, slope = 1) +
  aes(x = y_test, y = pred, color = cacs_pct) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Lipidomic Age"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw() 

# Age Acceleration Histogram

ggplot(graph_df) +
  aes(x = age_accl) +
  geom_histogram(bins = 100) +
  theme_bw() +
  labs(
    x = "Lipidomic Age Acceleration",
    y = "Frequency"
  )

# CACS vs Age Acceleration


mid = graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  summarise(mean = mean(cacs_pct)) %>%
  pull()

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = cacs_pct) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Lipidomic Age Acceleration",
    title = "CACS for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()

# SMURFS vs Age Acceleration

mid = mean(graph_df$smurfs)

graph_df %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = smurfs) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Lipidomic Age Acceleration",
    title = "SMURFs for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()


# Gensini vs Age Acceleration

mid = graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  summarise(mean = mean(gensini)) %>%
  pull()


graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = gensini) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "Lipidomic Age Acceleration",
    title = "Gensini for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()

# People who have low CACS score but high SMURFs
graph_df %>%
  mutate(cac_present = case_when(
    cacs_pct>0 ~ "present",
    cacs_pct==0 ~ "absent"
  )) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = smurfs, shape=cac_present) +
  geom_point() +
    labs(
      x = "Chronological Age",
      y = "Lipidomic Age Acceleration",
      title = "Resilient Individuals for Age Acceleration"
    ) + 
  scale_color_gradient2(midpoint=0, low="blue",
                     high="red" ) +
  theme_bw()

# Graph CACS vs Age Acceleration (Adjusted)
graph_df %>%
  #filter(cacsplus == "actionable") %>%
  #filter(is.na(cvd_FRS) == FALSE) %>%
  ggplot() +
  aes(x = age_accl_adj, y = cacs_pct, color=age) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "Age Acceleration (Adjusted)",
    y = "CACS Percentile",
  ) +
  scale_color_gradient(low="green", high="red" ) +
  theme_bw()

```

## CYTOF Age

```{r}


# Picks the first assay rlmSampleAllShort_H_batch
exp = longFormat(bioheart_mae[,,'CyTOF'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 1L)


df = data.frame(exp) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), names_from = rowname, values_from = value) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini)) %>%
  drop_na %>%
  # Normalise
  mutate(across(8:84, ~(.-min(.))/((max(.)-min(.)))))



bioheart_mae[['CyTOF']]

```

### Elastic Regression Approach

R\^2 = 0.2656321 RMSE = 9.563379

22 variables from 77 variables

```{r}

# X and Y datasets
y = df %>% 
    select(age) %>% 
    #scale(center = TRUE, scale = FALSE) %>% 
    as.matrix()

X = df %>% 
    select(-age, -primary, -smurfs, -cacs, -cacs_pct, -gensini, -colname) %>% 
    as.matrix()


# Train Test Split

set.seed(2022)
p = 0.7 
train = sample(1:nrow(X), nrow(X)*p)

X_train = X[train,]
y_train = y[train,]
X_test = X[-train,]
y_test = y[-train,]


# Model Building : Elastic Net Regression
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verboseIter = TRUE)


# Training Elastic Net Regression model
elastic_model = caret::train(age ~ .,
                             data = cbind(X[train,], "age" = y[train,]),
                             method = "glmnet",
                             tuneLength = 25,
                             trControl = control)


elastic_model

# Number of Coefficients
coef_matrix = coef(elastic_model$finalModel, elastic_model$bestTune$lambda)
coef_matrix
length(as.numeric(coef_matrix)[as.numeric(coef_matrix) != 0])

# Testing on Data Left-Out
elastic_model$bestTune
X_test = X[-train,]
y_test = y[-train,]

pred = predict(elastic_model, X_test)


metrics = bootstrap_metric(X_test, y_test,elastic_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")



```

### Using PCA and KDM

The first 14 principal components explain just 90% of the variance. The standard deviation is the square root of the eigenvalue; variances of PCs are eigenvalues of the covariance matrix.

R\^2 = 0.1443909 RMSE = 10.75873

<https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff> <https://online.stat.psu.edu/stat505/lesson/11/11.4#>:\~:text=We%20use%20the%20correlations%20between,square%20root%20of%20the%20eigenvalue. <https://stats.stackexchange.com/questions/133149/why-is-variance-instead-of-standard-deviation-the-default-measure-of-informati>

```{r}
pca_df = X[train,]
pca = prcomp(pca_df)
summary(pca)

screeplot(pca, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

cumpro = cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 5, col="blue", lty=5)
# abline(h = 0.63305, col="blue", lty=5)
abline(h = 0.9, col="red", lty=5)
legend("topleft", legend=c("Cut-off @ PC5", "90% Explained Variance"),
       col=c("blue", "red"), lty=5, cex=0.6)


```

-   KDM - uses chronological age, BM are a function of BA and CA.

<https://academic.oup.com/biomedgerontology/article/74/Supplement_1/S52/5625183>

```{r}
# PCA Axes for Subjects
axes = predict(pca, X[train,])
pca_dat = cbind(X[train,], axes, age = y[train,])

PCA_model = lm(age ~ PC1 + PC2 + PC3 + PC4, data.frame(pca_dat))
summary(PCA_model)


# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]
axes = predict(pca, X_test)

pred = predict(PCA_model, data.frame(axes))



metrics = bootstrap_metric(data.frame(axes), y_test,PCA_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")


```

```{r}
kdm_calc(
  pca,
)

```

### Using Random Forest

Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to "large p, small n" problems, and is able to account for correlation as well as interactions among features.

RMSE = 10.76234 R\^2 = 0.1105379

<https://www.sciencedirect.com/science/article/pii/S0888754312000626>

```{r}
ntree = 500

# Model Building : Random Forest
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verbose = TRUE)

rf_model =  caret::train(age ~ .,
                  data = cbind(X[train,], "age" = y[train,]),
                  method = 'rf',
                  tuneLength  = 15, 
                  trControl = control)

```

```{r}

# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]

pred = predict(rf_model, X_test)

metrics = bootstrap_metric(X_test, y_test, rf_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")


```

### Using Random BlockForest

-   Block selection randomization procedure (select subset of all M blocks with equal probability)
-   For each split, we sample fixed numbers of variables from each block separately. More precisely, for m=1,...,M we sample sqrt(p_m) variables from block m.

<https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2942-y#>

OOB prediction error (MSE): 130.6509 R squared (OOB): 0.1818328

```{r}

# Not as useful here since this is only a single block so I expect similar results to the random forest
blockforobj = blockfor(X[train,], y[train,], blocks = list(seq(1:ncol(X[train,]))), num.trees = 100)

```

```{r}

blockforobj
```

### Using Neural Network

iAge

2 fully connected layers with 5 nodes in each layer and tanh activation

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(256, activation = 'relu') %>%
  layer_dense(64, activation = 'relu') %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam(0.001)
)

```

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4931851/>

I copied an architecture to see if it did better. Pretty BAD.

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(500, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(250, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(100, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(50, activation = 'PReLU',kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adagrad(0.001)
)

```

L1 Regularisation + Batch Norm

```{r}
model = keras_model_sequential() %>%
  
  # Network architecture with L1 regularization and batch normalization
  layer_dense(units = 256, activation = "relu",
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 128, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 64, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 1) %>%

  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

Model with Dropout

```{r}

model = keras_model_sequential() %>%
  
  # Network architecture with 20% dropout
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1) %>%
  
  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

```{r}

# Train Model
fit1 = model %>%
  fit(
    x = X_train,
    y = y_train,
    epochs = 100,
    batch_size = 128,
    validation_split = 0.2,
    verbose = TRUE
  )

# Model Predict
pred = predict(model, X_test)

boxplot(abs(pred-y_test))

metrics = bootstrap_metric(X_test, y_test,model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

Autoencoder

```{r}

library(h2o)

```

### Using XGBoost

```{r}
library(xgboost)
xgb_train = xgb.DMatrix(data = X_train, label = y_train)
xgb_test = xgb.DMatrix(data = X_test, label = y_test)

# Watchlist
watchlist = list(train=xgb_train, test=xgb_test)


#fit XGBoost model
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)

#define final model
model_xgboost = xgboost(data = xgb_train, max.depth = 3, nrounds = 65, verbose = 0)
summary(model_xgboost)
pred = predict(model_xgboost, xgb_test)

boxplot(abs(pred-y_test))

data.frame(
  RMSE = RMSE(pred, y_test),
  Rsquare = R2(pred, y_test)
)

```

### Autoencoder

```{r}
library(h2o)
h2o.init()

features = as.h2o(X_train)

# Training Autoencoder on Features

ae1 = h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = 35,
  activation = "Tanh"
)

# Pulling Deep Features
ae1_codings = h2o.deepfeatures(ae1, features, layer = 1)
X_codings = as.data.frame(ae1_codings)

# Regression on Deep Features
slm = lm(y_train ~ ., cbind(y_train, X_codings))
summary(slm)

# Model Predict
test_features = as.h2o(X_test)
test_features = h2o.na_omit(test_features)
test_codings = h2o.deepfeatures(ae1, test_features, layer = 1)
test_codings_df = as.data.frame(test_codings)
pred = predict(slm, test_codings_df)
boxplot(abs(pred-y_test))


metrics = bootstrap_metric(test_codings_df, y_test, slm, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

## Combined

I want to see how the prediction improves if I use a BlockForest on Proteomic, Metabolomic and Lipidomics. In theory it should capture the interaction effects between different omics data.

```{r}
exp1 = longFormat(bioheart_mae[,,'Metabolomics'], # Does not do well
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 3L)


exp2 = longFormat(bioheart_mae[,,'Lipidomics_totals'], # Does well
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 2L)

exp3 = longFormat(bioheart_mae[,,'Proteomics'], # Does well
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"))

exp4 = longFormat(bioheart_mae[,,'CyTOF'],
                  colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                  i = 1L)

exp1_df = data.frame(exp1) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(
    primary = as.numeric(primary), 
    cacs = as.numeric(cacs), 
    cacs_pct = as.numeric(cacs_pct), 
    gensini = as.numeric(gensini)
    ) %>%
  left_join(dat_FRS %>% select(record_id, cvd_FRS, chd_FRS), 
            by = c("primary"="record_id"))


  

exp2_df = data.frame(exp2) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(
    primary = as.numeric(primary), 
    cacs = as.numeric(cacs), 
    cacs_pct = as.numeric(cacs_pct), 
    gensini = as.numeric(gensini)
    ) %>%
  left_join(dat_FRS %>% select(record_id, cvd_FRS, chd_FRS), 
            by = c("primary"="record_id"))



exp3_df = data.frame(exp3) %>%
  filter(!grepl("Repeat", colname)) %>%
  filter(primary != "79") %>%
  filter(primary != "Pool") %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(
    primary = as.numeric(primary), 
    cacs = as.numeric(cacs), 
    cacs_pct = as.numeric(cacs_pct), 
    gensini = as.numeric(gensini)
    ) %>%
  left_join(dat_FRS %>% select(record_id, cvd_FRS, chd_FRS), 
            by = c("primary"="record_id"))



exp4_df = data.frame(exp4) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(
    primary = as.numeric(primary), 
    cacs = as.numeric(cacs), 
    cacs_pct = as.numeric(cacs_pct), 
    gensini = as.numeric(gensini)
    ) %>%
  left_join(dat_FRS %>% select(record_id, cvd_FRS, chd_FRS), 
            by = c("primary"="record_id"))




ncol(exp1_df)
ncol(exp2_df)
exp3_df
exp4_df

metab_lipid_prot_cytof_merged_df = inner_join(exp1_df, exp2_df, by=c('primary', 'colname', 'age', 'smurfs', 'cacs', 'cacs_pct', 'gensini', 'cvd_FRS', 'chd_FRS')) %>%
  inner_join(exp3_df, by=c('primary', 'age', 'smurfs', 'cacs', 'cacs_pct', 'gensini', 'cvd_FRS', 'chd_FRS')) %>%
  inner_join(exp4_df, by=c('primary', 'colname', 'age', 'smurfs', 'cacs', 'cacs_pct', 'gensini', 'cvd_FRS', 'chd_FRS'))


metab_lipid_prot_merged_df = inner_join(exp1_df, exp2_df, by=c('primary', 'colname', 'age', 'smurfs', 'cacs', 'cacs_pct', 'gensini', 'cvd_FRS', 'chd_FRS')) %>%
  inner_join(exp3_df, by=c('primary', 'age', 'smurfs', 'cacs', 'cacs_pct', 'gensini', 'cvd_FRS', 'chd_FRS'))

lipid_prot_merged_df = inner_join(exp2_df, exp3_df, by=c('primary', 'age', 'smurfs', 'cacs', 'cacs_pct', 'gensini', 'cvd_FRS', 'chd_FRS'))
```

```{r}
# X and Y datasets
y = metab_lipid_prot_merged_df %>% 
    select(age) %>% 
    #scale(center = TRUE, scale = FALSE) %>% 
    as.matrix()

X = metab_lipid_prot_merged_df %>% 
    select(-age, -primary, -smurfs, -cacs, -cacs_pct, -gensini, -colname, -cvd_FRS, -chd_FRS) %>% 
    as.matrix()
```

```{r}
# Train Test Split

p = 0.7
train = sample(1:nrow(X), nrow(X)*p)

X_train =  X[train,]
y_train = y[train,]
X_test = X[-train,]
y_test = y[-train,]

```

### Elastic Regression

With 251 final variables R2: 8.332503 RMSE: 0.5935032 MAE: 6.395322

```{r}

# Parallel Computing
library(doParallel)
cl = makePSOCKcluster(5)
registerDoParallel(cl)



# Model Building : Elastic Net Regression
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verboseIter = TRUE)


# Training Elastic Net Regression model
elastic_model = caret::train(age ~ .,
                             data = cbind(X[train,], "age" = y[train,]),
                             method = "glmnet",
                             tuneLength = 25,
                             trControl = control)


elastic_model

# Number of Coefficients
coef_matrix = coef(elastic_model$finalModel, elastic_model$bestTune$lambda)
coef_matrix
length(as.numeric(coef_matrix)[as.numeric(coef_matrix) != 0])
rownames(coef(elastic_model$finalModel, elastic_model$bestTune$lambda))[as.numeric(coef_matrix) != 0]


# Testing on Data Left-Out
elastic_model$bestTune
X_test = X[-train,]
y_test = y[-train,]

pred = predict(elastic_model, X_test)
boxplot(abs(pred-y_test))

metrics = bootstrap_metric(X_test, y_test,elastic_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

## When you are done:
stopCluster(cl)

```

### Using PCA

```{r}
pca_df = X[train,]
pca = prcomp(pca_df)
summary(pca)

screeplot(pca, type = "l", npcs = 30, main = "Screeplot of the first 30 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

cumpro = cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 5, col="blue", lty=5)
# abline(h = 0.63305, col="blue", lty=5)
abline(h = 0.9, col="red", lty=5)
legend("topleft", legend=c("Cut-off @ PC5", "90% Explained Variance"),
       col=c("blue", "red"), lty=5, cex=0.6)


```

```{r}
# PCA Axes for Subjects
axes = predict(pca, X[train,])
pca_dat = cbind(X[train,], axes, age = y[train,])

PCA_model = lm(age ~ PC1 + PC2 + PC3 + PC4, data.frame(pca_dat))
summary(PCA_model)


# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]
axes = predict(pca, X_test)

pred = predict(PCA_model, data.frame(axes))


data.frame(
  RMSE = RMSE(pred, y_test),
  Rsquare = R2(pred, y_test)
)

metrics = bootstrap_metric(data.frame(axes), y_test, PCA_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")
```

### Using Random Forest

Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to "large p, small n" problems, and is able to account for correlation as well as interactions among features.

<https://www.sciencedirect.com/science/article/pii/S0888754312000626>

```{r}
ntree = 500

# Model Building : Random Forest
control = trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 5,
                       search = "random",
                       verbose = TRUE)

rf_model =  caret::train(age ~ .,
                         data = cbind(X[train,], "age" = y[train,]),
                         method = 'rf',
                         tuneLength  = 15, 
                         trControl = control)

```

```{r}

# Test on Test Data
X_test = X[-train,]
y_test = y[-train,]

pred = predict(rf_model, X_test)

metrics = bootstrap_metric(X_test, y_test, rf_model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Using Random BlockForest

-   Block selection randomization procedure (select subset of all M blocks with equal probability)
-   For each split, we sample fixed numbers of variables from each block separately. More precisely, for m=1,...,M we sample sqrt(p_m) variables from block m.

<https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2942-y#>

All 3 Experiments \* OOB prediction error (MSE): 119.0885 \* R squared (OOB): 0.2112567

Lipidomics + Proteomic Experiments \* OOB prediction error (MSE): 111.0757 \* R squared (OOB): 0.2219357

```{r}

# Setting up blocks
exp1_col = length(unique(exp1$rowname))
exp2_col = length(unique(exp2$rowname))
exp3_col = length(unique(exp3$rowname))
blocks = rep(1:3, times=c(exp1_col, exp2_col, exp3_col))
blocks = lapply(1:3, function(x) which(blocks==x))


```

```{r}

# Not as useful here since this is only a single block so I expect similar results to the random forest
blockforobj = blockfor(X_train, y_train, blocks = blocks, num.trees = 300)

```

```{r}

blockforobj$forest

pred = predict(blockforobj$forest, X_test)


metrics = bootstrap_metric(X_test, y_test,blockforobj$forest, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Using Neural Network

iAge

2 fully connected layers with 5 nodes in each layer and tanh activation

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(256, activation = 'relu') %>%
  layer_dense(64, activation = 'relu') %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adam(0.001)
)

```

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4931851/>

I copied an architecture to see if it did better. Pretty BAD.

```{r}


# Creating Model
model = keras_model_sequential() %>%
  layer_dense(500, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(250, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(100, activation = 'PReLU',
              kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(50, activation = 'PReLU',kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(1)



model %>% compile(
  loss = 'mean_absolute_error',
  optimizer = optimizer_adagrad(0.001)
)

```

L1 Regularisation + Batch Norm

```{r}
model = keras_model_sequential() %>%
  
  # Network architecture with L1 regularization and batch normalization
  layer_dense(units = 256, activation = "relu",
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 128, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 64, activation = "relu", 
              kernel_regularizer = regularizer_l2(0.001)) %>%
  #layer_batch_normalization() %>%
  layer_dense(units = 1) %>%

  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

Model with Dropout

Model with Dropout

```{r}

model = keras_model_sequential() %>%
  
  # Network architecture with 20% dropout
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1) %>%
  
  # Backpropagation
  compile(
    loss = 'mean_absolute_error',
    optimizer = optimizer_adam(0.001)
  )
```

```{r}

# Train Model
fit1 = model %>%
  fit(
    x = X_train,
    y = y_train,
    epochs = 100,
    batch_size = 128,
    validation_split = 0.2,
    verbose = TRUE
  )

# Model Predict
pred = predict(model, X_test)

boxplot(abs(pred-y_test))


metrics = bootstrap_metric(X_test, y_test,model, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")

```

### Autoencoder

```{r}
library(h2o)
h2o.init()

```
```{r}

features = as.h2o(X_train)
# Strange bug where special characters cause extra row created
features = h2o.na_omit(features)

```

```{r}


# Training Autoencoder on Features

ae1 = h2o.deeplearning(
  x = seq_along(features),
  training_frame = features,
  autoencoder = TRUE,
  hidden = 290,
  activation = "Tanh"
)

# Pulling Deep Features
ae1_codings = h2o.deepfeatures(ae1, features, layer = 1)
X_codings = as.data.frame(ae1_codings)

# Regression on Deep Features
slm = lm(y_train ~ ., cbind(y_train, X_codings))
summary(slm)

# Model Predict
test_features = as.h2o(X_test)
test_features = h2o.na_omit(test_features)
test_codings = h2o.deepfeatures(ae1, test_features, layer = 1)
test_codings_df = as.data.frame(test_codings)
pred = predict(slm, test_codings_df)
boxplot(abs(pred-y_test))


metrics = bootstrap_metric(test_codings_df, y_test, slm, 200)
paste(as.character(metrics[2,1])," (",as.character(metrics[2,2]),"-",as.character(metrics[2,3]), ")", sep="")
paste(as.character(metrics[3,1])," (",as.character(metrics[3,2]),"-",as.character(metrics[3,3]), ")", sep="")


```
### Grid Search Autoencoder


```{r}
hyper_grid = list(hidden = list(
  c(50),
  c(100), 
  c(300, 100, 300),
  c(100, 50, 100),
  c(450, 300, 250, 300, 450),
  c(250)
))


# Execute grid search
ae_grid = h2o.grid(
  algorithm = 'deeplearning',
  x = seq_along(features),
  training_frame = features,
  grid_id = 'autoencoder_grid',
  autoencoder = TRUE,
  activation = 'Tanh',
  hyper_params = hyper_grid,
  ignore_const_cols = FALSE,
  seed = 123
)

h2o.getGrid('autoencoder_grid', sort_by = 'mse', decreasing = FALSE)

```


### Graphs

```{r}
cbind(metab_lipid_prot_merged_df,y, pred, age_accl, age_accl_adj)

```


This paper here indicates what is resiliency and absence of CAC.

<https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.120.049057?cookieSet=1>

```{R}
graph_df 
dat_FRS %>%
  mutate(cacs = as.double(cacs))
```

```{r}

dat_FRS = dat_FRS %>%
  mutate(cacs = as.double(cacs),
         cacs_pct = as.double(cacs_pct))

# Calculating Age Acceleration
# negative age_accl means younger than chronological
# positive age_accl means older than chronological
pred = predict(elastic_model, X_test)
age_accl = pred-y_test
age_accl_adj = residuals(lm(age_accl ~ y_test))

graph_df = cbind(metab_lipid_prot_merged_df[-train,],y_test, pred, age_accl, age_accl_adj) %>%
  select(primary, age, smurfs, cacs, cacs_pct, gensini, y_test, pred, age_accl, age_accl_adj, cvd_FRS, chd_FRS) %>%
  mutate(cacsplus = case_when(
    cacs_pct > 75 ~ "actionable",
    cacs > 100 ~ "actionable",
    TRUE ~ "non-actionable"
  )) %>%
  # Add Phenotype Data
  left_join(dat_FRS, by=c("primary"="ID", "cacs"="cacs", "cacs_pct" = "cacs_pct","smurfs"="smurfs", "age"="age", "cvd_FRS"="cvd_FRS", "chd_FRS"="chd_FRS"), keep=FALSE)

# Try Whole Data
pred = predict(elastic_model, X)
age_accl = pred-y
age_accl_adj = residuals(lm(age_accl ~ y))

graph_df_full = cbind(metab_lipid_prot_merged_df, pred, age_accl_adj) %>%
  select(primary, age, smurfs, cacs, cacs_pct, gensini, pred, age_accl_adj, cvd_FRS, chd_FRS) %>%
  mutate(cacsplus = case_when(
    cacs_pct > 75 ~ "actionable",
    cacs > 100 ~ "actionable",
    TRUE ~ "non-actionable"
  )) %>%
  # Add Phenotype Data
  left_join(dat_FRS, by=c("primary"="ID", "cacs"="cacs", "cacs_pct" = "cacs_pct","smurfs"="smurfs", "age"="age", "cvd_FRS"="cvd_FRS", "chd_FRS"="chd_FRS"), keep=FALSE)







# Graph of Proteomic, Metabolomic and Lipidomics Age vs Chronological Age

mid = graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  summarise(mean = mean(cacs_pct)) %>%
  pull()

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() + 
  geom_abline(intercept = 0, slope = 1) +
  aes(x = y_test, y = pred, color = cacs_pct) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "PML Age"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw() 

# Graph of Age Acceleration vs Chronological Age
graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() +
  aes(x = age, y = age_accl_adj, color = cacs_pct) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "Chronological Age",
    y = "PML Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white", high="red" ) +
  theme_bw() 


# Age Acceleration Histogram

ggplot(graph_df) +
  aes(x = age_accl) +
  geom_histogram(bins = 100) +
  theme_bw() +
  labs(
    x = "PML Age Acceleration",
    y = "Frequency"
  )

# CACS vs Age Acceleration


mid = graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  summarise(mean = mean(cacs_pct)) %>%
  pull()

graph_df %>%
  filter(is.na(cacs_pct) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = cacs_pct) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "PML Age Acceleration",
    title = "CACS for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()

# SMURFS vs Age Acceleration

mid = mean(graph_df$smurfs)

graph_df %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = smurfs) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "PML Age Acceleration",
    title = "SMURFs for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()


# Gensini vs Age Acceleration

mid = graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  summarise(mean = mean(gensini)) %>%
  pull()


graph_df %>%
  filter(is.na(gensini) == FALSE) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = gensini) +
  geom_point() +
  labs(
    x = "Chronological Age",
    y = "PML Age Acceleration",
    title = "Gensini for Age Acceleration"
  ) +
  scale_color_gradient2(midpoint=mid, low="blue", mid="white",
                     high="red" ) +
  theme_bw()

library(plotly)

# People who have low CACS score but high SMURFs
graph = graph_df %>%
  mutate(cac_present = case_when(
    cacs_pct>0 ~ "present",
    cacs_pct==0 ~ "absent"
  )) %>%
  ggplot() +
  aes(x = y_test, y = age_accl, color = smurfs, shape=cac_present) +
  geom_point() +
    labs(
      x = "Chronological Age",
      y = "PML Age Acceleration",
      title = "Resilient Individuals for Age Acceleration"
    ) + 
  scale_color_gradient2(low="blue",
                     high="red" ) +
  theme_bw()

ggplotly(graph)

# CACS vs PML age
graph_df %>%
  filter(cacsplus == "actionable") %>%
  filter(is.na(cvd_FRS) == FALSE) %>%
  ggplot() +
  aes(x = cvd_FRS, y = cacs_pct) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "CVD FRS",
    y = "CACS Percentile (CACS+)",
  ) +
  theme_bw()

# Residuals Resilience Score
cacs_FRS_model = lm(cacs_pct ~ cvd_FRS, graph_df
                    #%>% filter(cacsplus == "actionable")
                    )
resilience_residuals = residuals(cacs_FRS_model)


# Orthogonal Distance Resilience Score

dist_point_line = function(a, slope, intercept) {
  x_0 = a[1]
  y_0 = a[2]
  d = abs(slope*x_0-y_0+intercept)/sqrt(slope^2+1)
  # positive resilience score if high risk low cacs
  if (y_0 <= slope*x_0+intercept) {
    return(d)
  } else {
    return(-d)
  }
  
}

resilience_orthogonals = graph_df %>%
  select(cvd_FRS, cacs_pct, cacsplus) %>%
  filter(cacsplus == "actionable") %>%
  filter(is.na(cvd_FRS) == FALSE) %>%
  apply(1, function(x) dist_point_line(as.numeric(x[1:2]), 
        slope = coefficients(cacs_FRS_model)[2], 
        intercept = coefficients(cacs_FRS_model)[1]))

# Graph Resilience vs Age Acceleration (Adjusted)
graph_df %>%
  #filter(cacsplus == "actionable") %>%
  filter(is.na(cvd_FRS) == FALSE) %>%
  mutate(resilience_residuals = resilience_residuals) %>%
  ggplot() +
  aes(x = age_accl_adj, y = resilience_residuals, color = age) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "Age Acceleration (Adjusted)",
    y = "Resiliency Score",
  ) +
  scale_color_gradient(low="green", high="red" ) +
  theme_bw()


# Graph Resilience vs Chronological Age
graph_df %>%
  filter(is.na(cvd_FRS) == FALSE) %>%
  mutate(resilience_residuals = resilience_residuals) %>%
  ggplot() +
  aes(x = age, y = resilience_residuals) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "Chronological Age",
    y = "Resiliency Score",
  ) +
  theme_bw()


# FRS against Age (Sense Check)

graph_df %>%
  ggplot() +
  aes(x = age, y =  cvd_FRS) +
  geom_point() +
  labs(
    x = "Age",
    y = "FRS"
  ) +
  theme_bw()

# Graph CACS vs Age Acceleration (Adjusted)
graph_df %>%
  #filter(cacsplus == "actionable") %>%
  filter(is.na(cvd_FRS) == FALSE) %>%
  ggplot() +
  aes(x = age_accl_adj, y = cacs_pct, color=age) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "Age Acceleration (Adjusted)",
    y = "CACS Percentile",
  ) +
  scale_color_gradient(low="green", high="red" ) +
  theme_bw()

summary(lm(cacs_pct ~ age_accl_adj, graph_df))

# Graph CACS vs FRS
graph_df %>%
  #filter(cacsplus == "actionable") %>%
  filter(is.na(cvd_FRS) == FALSE) %>%
  ggplot() +
  aes(x = cvd_FRS, y = cacs_pct) +
  geom_point() +
  stat_smooth(method='lm', col='red') +
  labs(
    x = "CVD FRS",
    y = "CACS Percentile",
  ) +
  scale_color_gradient(low="green", high="red" ) +
  theme_bw()

summary(lm(cacs_pct ~ age_accl_adj, graph_df))
summary(lm(resilience_residuals ~ graph_df %>% filter(is.na(cvd_FRS) == FALSE) %>% pull(age_accl_adj)))

summary(lm(cacs_pct ~ cvd_FRS, graph_df))
summary(lm(cacs_pct ~ cvd_FRS + age_accl_adj, graph_df))

summary(lm(cvd_FRS ~ age_accl_adj, graph_df))

temp_df =  graph_df_full %>% 
  filter(is.na(cvd_FRS) == FALSE)

cat_names = c('disc_1000','clinical_site','gender','ethcat','smurfs','cvhx_dm','diabetes_type','cvhx_htn','cvhx_hcl_sr_or_statin','cvhx_hcl_sr','cvhx_angina','cvhx_mi','cvhx_rhythm_svt','cvhx_rhythm_aflutter','cvhx_rhythm_af','cvhx_hf','cvhx_stent','cvhx_cabg','cvhx_heartsx_other','cvhx_cardiacprob_other','mhx_arthritis','mhx_arthritis_ra','mhx_arthritis_osteo','mhx_arthritis_gout','mhx_arthritis_other','mhx_osteoporosis','mhx_stroke','mhx_pad','mhx_dvt_pe','mhx_kidney','mhx_other','fh_ihd','fh_ihd_sex','fh_clottingdisorders','medication_yn','noac','warfarin','anti_coag','asa','clopidogrel','ticagrelor','prasugrel','anti_plt','statin','ezetimibe','fibrate','niacin','babr','plant_sterol','lipid_lowering_unknown','bblocker','ace_arb','ace_i','arb','ccb','diuretic','diur_loop','diur_thiazide','diur_k','diur_non_thiazide','diur_unk','anti_arrhythmic','ivabradine','ppi','antiinflammatory','anti_diab','metformin','sglt2','glp','dpp4','su','insulin','diab_unk','smoking_status','signif_smok','curr_smok','drinking_status','exposure_to_heavy_metals','pet_owner','sx_cp','sx_sob','sx_palp','sx_fatigue','sx_other','sx_acute','sx_ed','stat_rate_ctrl','ctca_ind_gen_cv_assessment','ctca_ind_sx','ctca_ind_ecg','ctca_ind_fh_ihd','ctca_ind_congenital_coronary','ctca_ind_surg','ctca_ind_other','bloods_fast','withdraw_no_fu','withdraw_full','od_50','od_75')

temp_df[,cat_names] = lapply(temp_df[,cat_names], factor)


cor(temp_df$cvd_FRS, temp_df$age_accl_adj)

summary(lm(gensini ~ age_accl_adj, graph_df))
```

ROC-AUC curve

```{r}

library(ROCit)

log_model_age_accl = glm(as.factor(cacsplus) ~ pred, data=temp_df, family="binomial")
log_model_frs = glm(as.factor(cacsplus) ~ age, data=temp_df, family="binomial")

rocit_frs = rocit(score = qlogis(log_model_frs$fitted.values), class = log_model_frs$y, method = "emp")
rocit_age_accl = rocit(score = qlogis(log_model_age_accl$fitted.values), class = log_model_age_accl$y, method = "emp")
summary(rocit_frs)
summary(rocit_age_accl)


plot(rocit_frs, col = c(1,"gray50"), legend = FALSE, YIndex = FALSE)
lines(rocit_age_accl$TPR ~ rocit_age_accl$FPR, col = 2, lwd = 2) 
legend("bottomright", col = c(1,2), c("FRS", "Biological Age"), lwd = 2)
title(main = "CACS+ vs CACS-")

## Plot ROC curve w/ CI
ciROC_frs = ciROC(rocit_frs, level = 0.95)
ciROC_age_accl = ciROC(rocit_age_accl, level = 0.95)
plot(ciROC_frs, col = 1, legend = FALSE)
lines(ciROC_age_accl$TPR~ciROC_age_accl$FPR, col = 2, lwd = 2)
lines(ciROC_age_accl$LowerTPR~ciROC_age_accl$FPR, col = 2, lty = 2)
lines(ciROC_age_accl$UpperTPR~ciROC_age_accl$FPR, col = 2, lty = 2)
legend("bottomright", c("FRS", "Biological Age", "95% CI (FRS)","95% CI (bAge)"),
       lty = c(1,1,2,2), col = c(1,2,1,2), lwd = c(2,2,1,1))
title(main = "Prediction of CACS+ vs CACS-")

## ROC comparison with pROC
library(pROC)
roc.test(as.factor(temp_df$cacsplus) ~ fitted(log_model_age_accl) + fitted(log_model_frs))




```

```{r}
library(jtools)

model = lm(pred ~ age + gender + ethcat + cvhx_htn + cvhx_dm + bmi + smoking_status + drinking_status + mhx_arthritis + mhx_osteoporosis + mhx_stroke + mhx_pad + mhx_dvt_pe + mhx_kidney, temp_df)
anova(model)
summary(model)

num_names = c("Chol","HDL","NT-proBNP","TG","CRP","LP(a)")

temp_df[,num_names] = lapply(temp_df[,num_names], as.numeric)

reduced_df = temp_df %>%
  drop_na(Chol,HDL,`NT-proBNP`,TG,CRP,`LP(a)`)

model2 = lm(pred ~ age + gender + ethcat + cvhx_htn + cvhx_dm + bmi + smoking_status + drinking_status + Chol + HDL + `NT-proBNP` + TG + CRP + `LP(a)`+ sbp + dbp, reduced_df)
confint(model)
summary(model)

groups = list(
  Demographic = c("Chronological Age", "Sex (ref: Male)", "Indigenous Australian (ref: European)", "Polynesian (ref: European)", "African (ref: European)", "Asian (ref: European)", "Indian (ref: European)", "Middle Eastern (ref: European)", "Hispanic (ref: European)", "Other / Dual Ethnicity (ref: European)"),
  Behaviours = c("BMI", "Never Smoked (ref: Current Smoker)", "Ex-Smoker (ref: Current Smoker)", "Ex-Drinker (ref: Current Drinker", "Non-Drinker (ref: Current Drinker", "Unknown (ref: Current Drinker"),
  `Medical History` = c("Hypertension (ref: None)","Diabetes mellitus (ref: None)", "Osteoarthritis (ref: None)", "Osteoporosis (ref: None)", "Stroke/TIA (ref: None)", "Peripheral arterial disease (ref: None)", "DVT/PE (ref: None)", "Kidney disease (ref: None)")
)

coefs = c("Chronological Age" = "age",
          "Sex (ref: Male)" = "gender2",
          "Indigenous Australian (ref: European)" = "ethcat2",
          "Polynesian (ref: European)" = "ethcat3",
          "African (ref: European)" = "ethcat4",
          "Asian (ref: European)" = "ethcat5",
          "Indian (ref: European)" = "ethcat6",
          "Middle Eastern (ref: European)" = "ethcat7",
          "Hispanic (ref: European)" = "ethcat8",
          "Other / Dual Ethnicity (ref: European)" = "ethcat9",
          "Hypertension (ref: None)" = "cvhx_htn1",
          "Diabetes mellitus (ref: None)" = "cvhx_dm1",
          "BMI" = "bmi",
          "Never Smoked (ref: Current Smoker)" = "smoking_status2",
          "Ex-Smoker (ref: Current Smoker)" = "smoking_status3",
          "Ex-Drinker (ref: Current Drinker" = "drinking_status2",
          "Non-Drinker (ref: Current Drinker" = "drinking_status3",
          "Unknown (ref: Current Drinker" = "drinking_status4",
          "Osteoarthritis (ref: None)" = "mhx_arthritis1",
          "Osteoporosis (ref: None)" = "mhx_osteoporosis1",
          "Stroke/TIA (ref: None)" = "mhx_stroke1",
          "Peripheral arterial disease (ref: None)" = "mhx_pad1",
          "DVT/PE (ref: None)" = "mhx_dvt_pe1",
          "Kidney disease (ref: None)" = "mhx_kidney1"
          )

plot_summs(model, coefs=coefs, groups=groups,colors ="Qual1")

```

### FRS Score w/ Age Acceleration

race (white, aa, chinese, or hispanic)
age (years) *
totchol (mg/dL) * <- ours is in mmol/L
hdl (mg/dL) *
sbp (mmHg) *
bp_med (Y/N)
* ace_i
* bblocker
* diuretic
* ccb
smoker (Y/N) *
diabetes (Y/N) *
bmi *
lipid_med (Y/N)
* statin
fh_heartattack (Y/N)
cac (Agatson) *

```{r}
dat_FRS

age_accl = pred-y_test

pheno_df = graph_df %>%
  left_join(dat_FRS, by=c("primary" = "record_id")) %>%
  # remove where gender not recorded
  drop_na(gender) %>%
  # limits of ascvd limited between 30 and 74
  filter((age.x<=74) & (age.x>=30)) %>%
  mutate(
    gender = case_when(
      gender==1 ~ 'male',
      gender==2 ~ 'female'
      ),
    HDL = HDL*38.67,
    Chol = Chol*38.67,
    bp_med = case_when(
      ace_arb == 1 ~ 1,
      bblocker == 1 ~ 1,
      diuretic == 1 ~ 1,
      ccb == 1 ~ 1,
      TRUE ~ 0
    )
  )
  

pheno_df = pheno_df %>%
  mutate(ascvd_10y_frs = ascvd_10y_frs(gender, age.x, HDL, Chol, sbp, bp_med, curr_smok, cvhx_dm),
         ascvd_10y_adjusted_frs = )


pheno_df['ascvd_10y_frs']

pheno_df %>%
  ggplot() +
  aes(x = ascvd_10y_frs, y = cacs_pct.x) +
  geom_point()


dat_FRS['sbp']
```

## Comparing Efficacy

```{r}

df = read.csv("R2-comparison.csv")
df %>% 
  ggplot() +
  aes(x = Type, y = R2) +
  facet_wrap(~ï..Assay, scales = "free_y") +
  theme_bw() +
  theme(axis.text.x=element_text(angle=40,hjust=1)) +
  geom_point() +
  geom_errorbar(aes(ymin=Lower, ymax=Upper), width=.1) +
  labs(
    x = "Assay & Machine Learning Model",
    y = "R2 Score",
    title = "Comparative Performance between ML Models in -omics assays"
  )
  

  


```


## Ensemble Model of Resilience


### Calculate Scores

race (white, aa, chinese, or hispanic)
age (years) *
totchol (mg/dL) * <- ours is in mmol/L
hdl (mg/dL) *
sbp (mmHg) *
bp_med (Y/N)
* ace_i
* bblocker
* diur_loop
* diur_k
* diur_unk
* ccb
smoker (Y/N) *
diabetes (Y/N) *
bmi *
lipid_med (Y/N)
* statin
fh_heartattack (Y/N)
cac (Agatson) *

#### SCORE2

```{r}

# ONLY FOR UP TO AND INCLUDING AGE OF 70
# Assumes a low-risk region
# sbp -> mmHg
# tchol -> mmol/L
# hdl -> mmol/L
# gender -> 'male' or 'female'

SCORE2_10yr= function(gender, age, curr_smok, sbp, dm, tchol, hdl) {
  cage = (age-60)/5
  csbp = (sbp-120)/20
  ctchol = (tchol-6)/1
  chdl = (hdl-1.3)/0.5
  dm = as.numeric(dm)
  curr_smok = as.numeric(dm)
  
  # Rescaling Factors
  male_RF = c(-0.5699, 0.7476)
  female_RF = c(-0.7380, 0.7019)
  
  # Linear Predictor + Calibration
  # Male
  if (gender == 'male') {
    beta_x = 0.3742*cage + 0.1458*ctchol - 0.2698*chdl + 0.2777*csbp + 
    0.6457*dm + 0.6012*curr_smok - 0.0281*cage*ctchol + 0.0426*cage*chdl +
    - 0.0255*cage*csbp - 0.0983*cage*dm - 0.0755*cage*curr_smok
    
    ten_yr_risk = 1-0.9605^(exp(beta_x))
    
    estimate = 1-exp(-exp(male_RF[1]+male_RF[2]*log(-log(1-ten_yr_risk))))
    
  # Female
  } else if (gender == 'female'){
    beta_x = 0.4648*cage + 0.1002*ctchol - 0.2606*chdl + 0.3131*csbp + 
    0.8096*dm + 0.7744*curr_smok - 0.0226*cage*ctchol + 0.0613*cage*chdl +
    - 0.0277*cage*csbp - 0.1272*cage*dm - 0.1088*cage*curr_smok
    
    ten_yr_risk = 1-0.9776^(exp(beta_x))
    
    estimate = 1-exp(-exp(female_RF[1]+female_RF[2]*log(-log(1-ten_yr_risk))))
  }
  
  return(estimate*100)
  
  
}

```

#### SCORE2-OP

```{r}

# ONLY FOR ABOVE 70 years old
# Assumes a low-risk region
# sbp -> mmHg
# tchol -> mmol/L
# hdl -> mmol/L
# gender -> 'male' or 'female'

SCORE2OP_10yr= function(gender, age, curr_smok, sbp, dm, tchol, hdl) {
  print(gender)
  cage = age-73
  csbp = sbp-150
  ctchol = tchol-6
  chdl = hdl-1.4
  dm = as.numeric(dm)
  curr_smok = as.numeric(dm)
  
  # Rescaling Factors
  male_RF = c(-0.34, 1.19)
  female_RF = c(-0.52, 1.01)
  
  # Linear Predictor + Calibration
  # Male
  if (gender == 'male') {
    beta_x = 0.0634*cage + 0.0850*ctchol - 0.3564*chdl + 0.0094*csbp + 
    0.4245*dm + 0.3524*curr_smok - 0.0073*cage*ctchol + 0.0091*cage*chdl +
    - 0.0005*cage*csbp - 0.0174*cage*dm - 0.0247*cage*curr_smok
    
    ten_yr_risk = 1-0.7576^(exp(beta_x-0.0929))
    
    estimate = 1-exp(-exp(male_RF[1]+male_RF[2]*log(-log(1-ten_yr_risk))))
    
  # Female
  } else if (gender == 'female'){
    beta_x = 0.0789*cage + 0.0605*ctchol - 0.3040*chdl + 0.0102*csbp + 
    0.6010*dm + 0.4921*curr_smok - 0.0009*cage*ctchol + 0.0154*cage*chdl +
    - 0.0004*cage*csbp - 0.0107*cage*dm - 0.0255*cage*curr_smok
    
    
    ten_yr_risk = 1-0.8082^(exp(beta_x-0.229))
    
    estimate = 1-exp(-exp(female_RF[1]+female_RF[2]*log(-log(1-ten_yr_risk))))
  }
  
  return(estimate*100)
  
  
}

```

#### FRS Risk

```{r}
calcFramingham91 <- function(df,
                             Age = age,
                             Sex = gender,
                             TChol = tc,
                             HDL = hdl,
                             SBP = sbp,
                             SmokingStatus = smoking_status,  
                             Diabetes = cvhx_dm,
                             Outcome = c("CVD", "CHD"),
                             Years = 5, #Can be 4 to 12 years
                             ...){

        
        
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # create new variables for calculation - might need to add additional measures
        
        row.names(df) <- df$ID
        data <- df %>% 
                
          
                # select required variables from dataframe
                select(all_of(Age), all_of(Sex), all_of(TChol), all_of(HDL), all_of(SBP), 
                       all_of(SmokingStatus), all_of(Diabetes)) %>%
                
                # rename BioHEART vars so function recognises them from input
                dplyr::rename(Sex = !!sym(Sex), 
                       Age = !!sym(Age),
                       TChol = !!sym(TChol),
                       HDL = !!sym(HDL),
                       SBP = !!sym(SBP),
                       SmokingStatus = !!sym(SmokingStatus),
                       Diabetes = !!sym(Diabetes)) %>%
        
                
                
                # create new variables required for function
                mutate(Female = if_else(Sex=="female", 1, 0),
                       CholHDLRatio = TChol/HDL,
                       Outcome = Outcome,
                       Years = Years) %>% 
        
        
                # remove obs with missing data
                drop_na() 
        
        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        # perform Framingham calculation
        
        a <- vector("double", nrow(data))
        mu <- vector("double", nrow(data))
        sigma <- vector("double", nrow(data))
        out <- vector("double", nrow(data))
        
        
        print(data)
        for(i in 1:nrow(data)) {
                if (is.na(Outcome)) {
                        stop("Outcome must be either 'CVD' or 'CHD")
                }
                else if (Outcome == "CVD") {
                        a[i] <- 18.8144 - 1.4032*log(data$SBP[i]) - 0.3899*as.numeric(data$SmokingStatus[i]) - 0.5390*log(data$CholHDLRatio[i])
                        
                        if (data$Female[i] == 1) {
                                mu[i] <- a[i] - 1.2146 - 1.4775*log(data$Age[i]) - 0.4733*data$Diabetes[i]
                        }
                        else {
                                mu[i] <- a[i] - 1.8443*log(data$Age[i]) - 0.3036*data$Diabetes[i]
                        }
                        
                        sigma[i] <- exp(0.6536 - 0.2402*mu[i])
                        out[i] <- 1 - exp(-exp((log(Years) - mu[i])/sigma[i]))
                        
                       
                } else if (Outcome == "CHD") {
                        a[i] <- 15.5305 - 0.9119*log(data$SBP[i]) - 0.2767*data$SmokingStatus[i] - 0.7181*log(data$CholHDLRatio[i])
                        
                        if (data$Female[i] == TRUE) {
                                mu[i] <- a[i] + 28.4441 - 15.938*log(data$Age[i]) + 1.8515*(log(data$Age[i])^2) - 0.3758*data$Diabetes[i]
                                
                        } else {
                                mu[i] <- a[i] - 1.4792*log(data$Age[i]) - 0.1759*data$Diabetes[i]
                                
                        }
                        
                        sigma[i] <- exp(0.9145 - 0.2784*mu[i])
                        out[i] <- 1 - exp(-exp((log(Years) - mu[i])/sigma[i]))
                        
                }
                
        }
        
        Xout <- data.frame(ID = row.names(data),
                           Score.Type = paste0("FRS (91) ", Years, " Year ", Outcome, " Risk"),
                           Risk.Prob = out)
        
        return(Xout)
        
}

```

#### All Score Calculations

ethcat
1 = European
2 = Indigenous Australia
3 = Polynesian
4 = African
5 = Asian
6 = Indian
7 = Middle Eastern
8 = Hispanic
9 = Other / Dual Ethnicity

```{r}

dat_FRS

pheno_df = dat_FRS %>%
  # remove where gender or cac or cacs_pct not recorded
  drop_na(gender, cacs, cacs_pct) %>%
  # missing value encoded as .
  filter(cacs_pct != ".") %>%
  # limits of ascvd limited between 30 and 74
  filter((age<=74) & (age>=30)) %>%
  mutate(
    # refactor gender
    gender = case_when(
      gender==1 ~ 'male',
      gender==2 ~ 'female'
      ),
    # convert from mmol/L to mg/dL
    HDL_mgdl = HDL*38.67,
    Chol_mgdl = Chol*38.67,
    # new variable presence or absence of blood pressure meds
    bp_med = case_when(
      ace_arb == 1 ~ 1,
      bblocker == 1 ~ 1,
      diuretic == 1 ~ 1,
      ccb == 1 ~ 1,
      TRUE ~ 0
    ),
    # new variable presence or absence of lipid lowering meds
    lipid_med = case_when(
      statin == 1 ~ 1,
      ezetimibe == 1 ~ 1,
      fibrate == 1 ~ 1,
      niacin == 1 ~ 1,
      babr == 1 ~ 1,
      plant_sterol == 1 ~ 1,
      lipid_lowering_unknown == 1 ~ 1,
      TRUE ~ 0
    ),
    # assume no history of IHD if NA
    fh_ihd = case_when(
      fh_ihd == 1 ~ 1,
      fh_ihd == 0 ~ 0,
      is.na(fh_ihd) ~ 0
    ),
    # CACS as number
    cacs = as.numeric(cacs),
    cacs_pct = as.numeric(cacs_pct),
    # ACCAHA Ethnicity
    accaha_eth = case_when(
      ethcat == 1 ~ 'white',
      ethcat == 4 ~ 'aa',
      TRUE ~ 'other'
    ),
    # Unresolved: MESA only contains 4 ethnicity categories, what do we do with other?
    mesa_eth = case_when(
      ethcat == 1 ~ 'white',
      ethcat == 2 ~ 'aa',
      ethcat == 3 ~ 'white', # contentious Polynesian
      ethcat == 4 ~ 'aa',
      ethcat == 5 ~ 'chinese',
      ethcat == 7 ~ 'white',
      ethcat == 8 ~ 'hispanic',
      ethcat == 9 ~ 'white', # contentious mixed/Other
      TRUE ~ 'white' # if NA assume white
    ),
    
  )

pheno_df = pheno_df %>%
  rowwise() %>%
  mutate(
    # FRS Score
    ascvd_10y_frs = ascvd_10y_frs(gender, age, HDL_mgdl, Chol_mgdl, sbp, bp_med, curr_smok, cvhx_dm),
    # ASCVD (Pooled Cohort Equations - USA) w/ Requirements
    # 20 < age < 79 
    # 130 < totchol < 320
    # 20 < hdl < 100
    # 90 < sbp < 200
    ascvd_10y_accaha = ascvd_10y_accaha(accaha_eth, gender, age, Chol_mgdl, HDL_mgdl, sbp, bp_med, curr_smok, cvhx_dm),
    # MESA CHD
    chd_10y_mesa = chd_10y_mesa(mesa_eth, gender, age, Chol_mgdl, HDL_mgdl,lipid_med, sbp, bp_med, curr_smok, cvhx_dm, fh_ihd),
    # MESA CHD w/ CAC
    chd_10y_mesa_cac = chd_10y_mesa_cac(mesa_eth, gender, age, Chol_mgdl, HDL_mgdl,lipid_med, sbp, bp_med, curr_smok, cvhx_dm, fh_ihd, cacs),
    # SCORE2 <= 70 year olds
    SCORE2_10yr = SCORE2_10yr(gender, age, curr_smok, sbp, cvhx_dm, Chol, HDL),
    # SCORE2-OP > 70 year olds
    SCORE2OP_10yr= SCORE2OP_10yr(gender, age, curr_smok, sbp, cvhx_dm, Chol, HDL),
    ) %>%
  mutate(
    # Use the right SCORE2 based on age
    SCORE2 = case_when(
      age <= 70 ~ SCORE2_10yr,
      age > 70 ~ SCORE2OP_10yr
    )
  )

score_df = pheno_df %>%
  dplyr::select(ID, cacs, cacs_pct, ascvd_10y_frs, ascvd_10y_accaha, chd_10y_mesa, SCORE2)
# Picked MESA without CAC since CAC will be the y-axis when calculating residuals
# 112 missing values out of range but no particular pattern in its distribution
score_df %>%
  mutate(
    na = case_when(
      is.na(ascvd_10y_accaha) ~ TRUE,
      !is.na(ascvd_10y_accaha) ~ FALSE
    )
  ) %>%
  #filter(na == TRUE) %>%
  ggplot() +
  aes(x = cacs_pct, y = ".") +
  geom_boxplot() +
  geom_jitter(aes(color = na, alpha = 0.2))


score_df %>%
  vis_miss()

score_df = score_df %>% drop_na()

score_df

```

### Calculate Residuals

```{r}
# Calculate vertical residuals of linear model regressing CACS %tile on score
calculate_residuals = function(cacs_pct, score) {
  lm = lm(cacs_pct ~ score - 1) # forcing through zero
  return(residuals(lm))
}

# Calculate studentised vertial residuals of linear model regressing CACS %tile on score
studentised_residuals = function(cacs_pct, score) {
  lm = lm(cacs_pct ~ score - 1) # forcing through zero
  return(rstudent(lm))
}

# Calculate diagonal residuals of linear model regressing CACS%tile on score

dist_point_line = function(a, slope, intercept) {
  x_0 = a[1]
  y_0 = a[2]
  d = abs(slope*x_0-y_0+intercept)/sqrt(slope^2+1)
  # positive resilience score if high risk low cacs
  if (y_0 <= slope*x_0+intercept) {
    return(d)
  } else {
    return(-d)
  }
  
}

calculate_residuals = function(cacs_pct, score) {
  lm = lm(cacs_pct ~ score)
  residuals = apply(cbind(score, cacs_pct), 1, dist_point_line,
        slope = coefficients(lm)[2], 
        intercept = coefficients(lm)[1]
  )
  return(residuals)
}

```

```{r}

score_df = score_df %>%
  mutate(ln_cacs = log(cacs+1)) %>%
  mutate(
    res_ascvd_10y_frs = calculate_residuals(ln_cacs, ascvd_10y_frs),
    res_ascvd_10y_accaha = calculate_residuals(ln_cacs, ascvd_10y_accaha),
    res_chd_10y_mesa = calculate_residuals(ln_cacs, chd_10y_mesa),
    res_SCORE2 = calculate_residuals(ln_cacs, SCORE2)
  ) %>%
  mutate(
    studres_ascvd_10y_frs = studentised_residuals(ln_cacs, ascvd_10y_frs),
    studres_ascvd_10y_accaha = studentised_residuals(ln_cacs, ascvd_10y_accaha),
    studres_chd_10y_mesa = studentised_residuals(ln_cacs, chd_10y_mesa),
    studres_SCORE2 = studentised_residuals(ln_cacs, SCORE2)
  )

# Showing how many subjects have residuals of non-uniform sign
score_df %>%
  mutate(s = res_ascvd_10y_frs * res_ascvd_10y_accaha * res_chd_10y_mesa * res_SCORE2) %>%
  filter(s < 0)

```

### Classifying Residuals

#### Classification Functions

```{r}
# Type 1 Classification - Absolute quantiles same for resilient and susceptible (DEPRECATED)

# Determine class of subject
# Find the quantile according to the percentile cutoff for absolute residuals
# Absolute residuals below this quantile are considered reference
# Residuals more negative than the quantile are resilient
# Residuals more positive than the quantile are considered susceptible
calculate_classes = function(residuals, lower_quantile, upper_quantile, reference_cutoff = 0.2) {
  cohort_split = function(x) {
      if (abs(x) <= lower_quantile){
        return(as.factor("reference"))
      } else if (x < -upper_quantile) {
        return(as.factor("resilient"))
      } else if (x > upper_quantile) {
        return(as.factor("susceptible"))
      } else {
        return(as.factor("ignore"))
      }
    }
  return(sapply(residuals, FUN = cohort_split))
}

reference_cutoff = 0.2
lower_quantile_frs = quantile(abs(score_df$res_ascvd_10y_frs), probs = reference_cutoff)
upper_quantile_frs = quantile(abs(score_df$res_ascvd_10y_frs), probs = 1-reference_cutoff)

lower_quantile_accaha = quantile(abs(score_df$res_ascvd_10y_accaha), probs = reference_cutoff)
upper_quantile_accaha = quantile(abs(score_df$res_ascvd_10y_accaha), probs = 1-reference_cutoff)

lower_quantile_mesa = quantile(abs(score_df$res_chd_10y_mesa), probs = reference_cutoff)
upper_quantile_mesa = quantile(abs(score_df$res_chd_10y_mesa), probs = 1-reference_cutoff)

lower_quantile_SCORE2 = quantile(abs(score_df$res_SCORE2), probs = reference_cutoff)
upper_quantile_SCORE2 = quantile(abs(score_df$res_SCORE2), probs = 1-reference_cutoff)

score_df = score_df %>%
  mutate(
    class_ascvd_10y_frs = calculate_classes(res_ascvd_10y_frs, lower_quantile_frs, upper_quantile_frs),
    class_ascvd_10y_accaha = calculate_classes(res_ascvd_10y_accaha, lower_quantile_accaha, upper_quantile_accaha),
    class_chd_10y_mesa = calculate_classes(res_chd_10y_mesa, lower_quantile_mesa, upper_quantile_mesa),
    class_SCORE2 = calculate_classes(res_SCORE2, lower_quantile_SCORE2, upper_quantile_SCORE2)
  )

```

``` {r}
# Type 2 Classification - Absolute quantiles different for resilient and susceptible
# Find the quantiles for susceptible and resilient based on only residuals either positive or negative
calculate_classes_2 = function(residuals, lower_quantile_resilient, upper_quantile_resilient, lower_quantile_susceptible, upper_quantile_susceptible) {
  
  cohort_split = function(x) {
      if ((x >= upper_quantile_resilient) & (x <= lower_quantile_susceptible)) {
        return(as.factor("reference"))
      } else if (x < lower_quantile_resilient) {
        return(as.factor("resilient"))
      } else if (x > upper_quantile_susceptible){
        return(as.factor("susceptible"))
      } else {
        return(as.factor("ignore"))
      }
  }
  return(sapply(residuals, FUN = cohort_split))
  }

```

#### Method 1: By Each Score

This first method took set of residuals from each of the difference scores and classifies them separately. The intention was then to take the majority vote of classifications.

```{r}
apply_classes = function(score_df, reference_cutoff = 0.20) {
    lower_quantile_resilient_frs = quantile(score_df$res_ascvd_10y_frs[score_df$res_ascvd_10y_frs < 0], probs =   reference_cutoff)
  upper_quantile_resilient_frs = quantile(score_df$res_ascvd_10y_frs[score_df$res_ascvd_10y_frs < 0], probs =   1-reference_cutoff/2)
  lower_quantile_susceptible_frs = quantile(score_df$res_ascvd_10y_frs[score_df$res_ascvd_10y_frs > 0], probs =   reference_cutoff/2)
  upper_quantile_susceptible_frs = quantile(score_df$res_ascvd_10y_frs[score_df$res_ascvd_10y_frs > 0], probs =   1-reference_cutoff)
  cat(lower_quantile_resilient_frs, ", ", upper_quantile_resilient_frs, ", ", lower_quantile_susceptible_frs, ", ", upper_quantile_susceptible_frs, "\n")

  
  lower_quantile_resilient_accaha = quantile(score_df$res_ascvd_10y_accaha[score_df$res_ascvd_10y_accaha < 0], probs =   reference_cutoff)
  upper_quantile_resilient_accaha = quantile(score_df$res_ascvd_10y_accaha[score_df$res_ascvd_10y_accaha < 0], probs =   1-reference_cutoff/2)
  lower_quantile_susceptible_accaha = quantile(score_df$res_ascvd_10y_accaha[score_df$res_ascvd_10y_accaha > 0], probs =   reference_cutoff/2)
  upper_quantile_susceptible_accaha = quantile(score_df$res_ascvd_10y_accaha[score_df$res_ascvd_10y_accaha > 0], probs =   1-reference_cutoff)
  
  lower_quantile_resilient_mesa = quantile(score_df$res_chd_10y_mesa[score_df$res_chd_10y_mesa < 0], probs =   reference_cutoff)
  upper_quantile_resilient_mesa = quantile(score_df$res_chd_10y_mesa[score_df$res_chd_10y_mesa < 0], probs =   1-reference_cutoff/2)
  lower_quantile_susceptible_mesa = quantile(score_df$res_chd_10y_mesa[score_df$res_chd_10y_mesa > 0], probs =   reference_cutoff/2)
  upper_quantile_susceptible_mesa = quantile(score_df$res_chd_10y_mesa[score_df$res_chd_10y_mesa > 0], probs =   1-reference_cutoff)
  
  lower_quantile_resilient_SCORE2 = quantile(score_df$res_SCORE2[score_df$res_SCORE2 < 0], probs = reference_cutoff)
  upper_quantile_resilient_SCORE2 = quantile(score_df$res_SCORE2[score_df$res_SCORE2 < 0], probs = 1-reference_cutoff/2)
  lower_quantile_susceptible_SCORE2 = quantile(score_df$res_SCORE2[score_df$res_SCORE2 > 0], probs = reference_cutoff/2)
  upper_quantile_susceptible_SCORE2 = quantile(score_df$res_SCORE2[score_df$res_SCORE2 > 0], probs = 1-reference_cutoff)
  
  
  score_df = score_df %>%
    mutate(
      class_ascvd_10y_frs = calculate_classes_2(res_ascvd_10y_frs, lower_quantile_resilient_frs,   upper_quantile_resilient_frs, lower_quantile_susceptible_frs, upper_quantile_susceptible_frs),
      class_ascvd_10y_accaha = calculate_classes_2(res_ascvd_10y_accaha, lower_quantile_resilient_accaha,   upper_quantile_resilient_accaha, lower_quantile_susceptible_accaha, upper_quantile_susceptible_accaha),
      class_chd_10y_mesa = calculate_classes_2(res_chd_10y_mesa, lower_quantile_resilient_mesa,   upper_quantile_resilient_mesa, lower_quantile_susceptible_mesa, upper_quantile_susceptible_mesa),
      class_SCORE2 = calculate_classes_2(res_SCORE2, lower_quantile_resilient_SCORE2, upper_quantile_resilient_SCORE2,   lower_quantile_susceptible_SCORE2, upper_quantile_susceptible_SCORE2)
    )
  return(score_df)
}


score_df = apply_classes(score_df, reference_cutoff = 0.20)
write.csv(score_df, "~/desktop/score_df.csv", row.names = FALSE)
```

Visualise

```{r}
library(tidyr)
long_score_df = score_df %>%
  dplyr::select(-starts_with("res")) %>%
  pivot_longer(cols = c("ascvd_10y_frs", "ascvd_10y_accaha", "chd_10y_mesa", "SCORE2"), names_to = "score_type", values_to = "score") %>%
  pivot_longer(cols = starts_with("class_"), names_to = "class_type", values_to = "classification") %>%
  mutate(class_type = gsub("class_", "", class_type)) %>%
  filter(score_type == class_type) %>%
  mutate(score_type = as.factor(score_type)) %>%
  dplyr::select(-class_type)

# create a list of unique score types
score_types = unique(long_score_df$score_type)

# create a plot for each score type
plots = lapply(score_types, function(score_type) {
  # subset the data for the current score type
  df_subset = long_score_df[long_score_df$score_type == score_type,]
  print(df_subset)
  # create the plot
  #ggplot(df_subset, aes(x = score, y = cacs_pct)) +
  ggplot(df_subset, aes(x=score, y=ln_cacs)) +
    geom_point(aes(color = classification)) +
    geom_smooth(formula = y ~ x - 1, method = 'lm', se = FALSE) +
    theme_bw() +
    labs(title = paste("CACS vs", score_type),
         x = "Score Value", y = "ln(CACS+1)")
})


par(mfrow=c(2,2))
lapply(score_types, function(score_type) {
  
  df_subset = long_score_df[long_score_df$score_type == score_type,]
  df_subset = column_to_rownames(df_subset, var = "ID")
  print(df_subset)
  lm = lm(score ~ ln_cacs, df_subset)
  plot(lm, 4, main = score_type)
  
})


# arrange the plots in a grid
p = gridExtra::grid.arrange(grobs = plots, ncol = 2, nrow = 2)
ggsave("~/Desktop/myplot.png", plot = p, width = 12, height = 8, dpi = 300)

```


```{r}

# Majority consensus between scores for grouping
score_df = score_df %>%
  rowwise() %>%
  mutate(
    
    consensus_class = as.factor(names(which.max(table(c(class_ascvd_10y_frs, class_ascvd_10y_accaha, class_chd_10y_mesa, class_SCORE2)))))
  )



# Following function is used to check which rows do not have some sort of majority concordance.
## define function to check if at least 3 out of 4 columns are equal
check_equal <- function(x) {
  sum(duplicated(x)) >= 2
}

## apply function to each row of the data frame and get row indices where condition is not met
rows_not_equal <- which(!apply(score_df[, c("class_ascvd_10y_frs", "class_ascvd_10y_accaha", "class_chd_10y_mesa", "class_SCORE2")], 1, check_equal))
score_df[rows_not_equal,]
pheno_df %>%
  filter(ID %in% c(score_df[rows_not_equal, 'ID'] %>% pull()))



table(score_df$consensus_class)
```

#### Method 2: Average Studentised Residual

Externally studentised residuals. Importantly we assume the normality assumptions of the original model is met.

https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model

```{r}
score_df = score_df %>%
  rowwise() %>%
  mutate(avg_studres = mean(c(studres_ascvd_10y_frs, studres_ascvd_10y_accaha, studres_chd_10y_mesa, studres_SCORE2)))

reference_cutoff = 0.2

lower_quantile_resilient = quantile(score_df$avg_studres[score_df$avg_studres < 0], probs =   reference_cutoff)
upper_quantile_resilient = quantile(score_df$avg_studres[score_df$avg_studres < 0], probs =   1-reference_cutoff/2)
lower_quantile_susceptible = quantile(score_df$avg_studres[score_df$avg_studres > 0], probs =   reference_cutoff/2)
upper_quantile_susceptible = quantile(score_df$avg_studres[score_df$avg_studres > 0], probs =   1-reference_cutoff)

cat(lower_quantile_resilient, ", ", upper_quantile_resilient, ", ", lower_quantile_susceptible, ", ", upper_quantile_susceptible, "\n")


score_df = score_df %>%
  mutate(
    consensus_class = calculate_classes_2(avg_studres, 
                                            lower_quantile_resilient, 
                                            upper_quantile_resilient, 
                                            lower_quantile_susceptible, 
                                            upper_quantile_susceptible)
  )

score_df


```
```{r}
long_score_df = score_df %>%
  dplyr::select(-starts_with("studres")) %>%
  pivot_longer(cols = c("ascvd_10y_frs", "ascvd_10y_accaha", "chd_10y_mesa", "SCORE2"), names_to = "score_type", values_to = "score") %>%
  mutate(score_type = as.factor(score_type))


# create a list of unique score types
score_types = unique(long_score_df$score_type)

# create a plot for each score type
plots = lapply(score_types, function(score_type) {
  # subset the data for the current score type
  df_subset = long_score_df[long_score_df$score_type == score_type,]
  print(df_subset)
  # create the plot
  #ggplot(df_subset, aes(x = score, y = cacs_pct)) +
  ggplot(df_subset, aes(x=score, y=ln_cacs)) +
    geom_point(aes(color = consensus_class)) +
    geom_smooth(formula = y ~ x - 1, method = 'lm', se = FALSE) +
    theme_bw() +
    labs(title = paste("CACS vs", score_type),
         x = "Score Value", y = "ln(CACS+1)")
})

# arrange the plots in a grid
p = gridExtra::grid.arrange(grobs = plots, ncol = 2, nrow = 2)
ggsave("~/Desktop/myplot.png", plot = p, width = 12, height = 8, dpi = 300)
```

### Comparing Classes

#### Metabolomics

#### 

#### Differential Expression Analysis

```{r}
# Metabolomics
# Picks the first assay rlmSampleAllShort_H_batch
metab_exp = longFormat(bioheart_mae[,,'Metabolomics'],
                 colDataCols = c("gender","age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 3L)

metab_df = data.frame(metab_exp) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, gender, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  filter(if_all(everything(), ~ .!=".")) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini), primary = as.numeric(primary)) %>%
  # Remove duplicate rows, .keep_all <- keeps all of the rest of columns
  distinct(primary, .keep_all = TRUE) %>%
  drop_na() #%>%
  #mutate(across(8:60, ~(.-min(.))/((max(.)-min(.)))))

graph_df = score_df %>%
  dplyr::select(ID, consensus_class) %>%
  inner_join(metab_df, by=c('ID'='primary')) # %>%
  #filter(consensus_class %in% c("resilient", "reference"))

graph_df %>%
  pivot_longer(cols = -c(ID, gender, consensus_class, colname, age, smurfs, cacs, cacs_pct, gensini), names_to = "metabolite", values_to = "value") %>%
  ggplot() +
  aes(x = metabolite, y = value, colour = consensus_class) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```




```{r}


# PCA Analysis
metab_pca = graph_df %>%
  select(-c(ID, consensus_class, colname, gender, age, smurfs, cacs, cacs_pct, gensini)) %>%
  #scale() %>%
  prcomp()

fviz_eig(metab_pca, addlabels = TRUE)

fviz_pca_ind(metab_pca,
             geom.ind = "point",
             col.ind = graph_df %>% select(consensus_class) %>% pull(),
             palette = c("#00AFBB", "#E7B800", "#FC4E07", "#000000"),
             addEllipses = TRUE,
             legend.title = "Groups")

metab_pca_limited = graph_df %>%
  filter(gender == 1) %>%
  filter(age > 60) %>%
  select(-c(ID, consensus_class, colname, gender, age, smurfs, cacs, cacs_pct, gensini)) %>%
  #scale() %>%
  prcomp()


fviz_pca_ind(metab_pca_limited,
             geom.ind = "point",
             col.ind = graph_df %>% filter(gender == 1) %>% filter(age > 60) %>% select(consensus_class) %>% pull(),
             palette = c("#00AFBB", "#E7B800", "#FC4E07", "#000000"),
             addEllipses = TRUE,
             legend.title = "Groups")

fviz_pca_ind(metab_pca_limited,
             geom.ind = "point",
             col.ind = graph_df %>% filter(gender == 1) %>% filter(age > 60) %>% select(cacs_pct) %>% pull(),
             gradient.cols = c("green", "red"),
             legend.title = "Groups")
```


```{r}
filtered_graph_df = graph_df #%>%
  #filter(gender == 1) %>%
  #filter(age > 60) 
lm_df = filtered_graph_df %>%
  dplyr::select(-c(ID, consensus_class, colname, gender, age, smurfs, cacs, cacs_pct, gensini)) %>%
  t()

rownames(lm_df)
dim(lm_df)
dim(design)
design = model.matrix(~ consensus_class, filtered_graph_df)
fit = lmFit(lm_df, design)
fit = eBayes(fit)
topTable(fit)

#CM = makeContrasts(consensus_classresilient = consensus_classresilient - consensus_classreference, levels = design)
#CM = makeContrasts(consensus_classresilient = consensus_classresilient - consensus_classsusceptible, levels = design)
CM = makeContrasts(consensus_classresilient = consensus_classsusceptible - consensus_classreference, levels = design)
constrast_fit = contrasts.fit(fit, contrast = CM)
constrast_fit = eBayes(constrast_fit)
topTable(constrast_fit)

table(filtered_graph_df$consensus_class)


```

#### Univariate Analysis

```{r}
filtered_graph_df = filtered_graph_df %>%
  select(-c(ID, colname, gender, age, smurfs, cacs, cacs_pct, gensini)) %>%
  filter(consensus_class %in% c("resilient", "reference"))
  
filtered_graph_df$consensus_class = droplevels(filtered_graph_df$consensus_class)
# Explicitly set reference level to reference class
filtered_graph_df$consensus_class = relevel(filtered_graph_df$consensus_class, ref = "reference")
lapply(models, summary)

models = apply(filtered_graph_df[,-1], 
               MARGIN=2,
               FUN = function(x) glm(filtered_graph_df$consensus_class ~ x, family = "binomial"))

# Extract the p-value for each variable in the glm models using lapply()
results_list = lapply(models, function(model) {
  p_value = summary(model)$coefficients[2, "Pr(>|z|)"]
  odds_ratio = exp(summary(model)$coefficients[2, "Estimate"])
  data.frame(p_value = p_value, odds_ratio = odds_ratio, stringsAsFactors = FALSE)
})

# Combine the data frames in the results list into a single data frame using do.call() and rbind()
results_df = do.call(rbind, results_list)

# Adjust p_value
adj_p = p.adjust(results_df$p_value, method="fdr")

# Print the results
results_df = results_df %>%
  mutate(adj_p_value = adj_p) %>%
  arrange(p_value)
  

knitr::kable(results_df)

```

##### Multivariate Analysis

```{r}
mlr_df = 
filtered_graph_df %>%
  select(-c(colname, gender, age, smurfs, cacs, cacs_pct, gensini)) %>%
  column_to_rownames(var = "ID") %>%
  filter(consensus_class %in% c("resilient", "reference"))


model = glm(consensus_class ~ ., mlr_df, family = binomial)
summary(model)

levels(filtered_graph_df$consensus_class)
```

```{r}

# Picks the first assay rlmSampleAllShort_H_batch
lipid_species_exp = longFormat(bioheart_mae[,,'Lipidomics_species'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 2L)

lipid_totals_exp = longFormat(bioheart_mae[,,'Lipidomics_totals'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"),
                 i = 2L)


lipid_species_df = data.frame(lipid_species_exp) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini), primary = as.numeric(primary))


lipid_totals_df = data.frame(lipid_totals_exp) %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, colname, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini), primary = as.numeric(primary))


bioheart_mae[['Lipidomics_species']]



score_df %>%
  select(ID, consensus_class) %>%
  inner_join(lipid_totals_df, by=c('ID'='primary')) %>%
  filter(consensus_class %in% c("resilient", "reference")) %>%
  pivot_longer(cols = -c(ID, consensus_class, colname, age, smurfs, cacs, cacs_pct, gensini), names_to = "lipid_total", values_to = "value") %>%
  ggplot() +
  aes(x = lipid_total, y = value, colour = consensus_class) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))



score_df %>%
  select(ID, consensus_class) %>%
  inner_join(lipid_species_df, by=c('ID'='primary')) %>%
  filter(consensus_class %in% c("resilient", "reference")) %>%
  pivot_longer(cols = -c(ID, consensus_class, colname, age, smurfs, cacs, cacs_pct, gensini), names_to = "lipid_species", values_to = "value") %>%
  ggplot() +
  aes(x = lipid_species, y = value, colour = consensus_class) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```
#### Proteomics

```{r}

prot_exp = longFormat(bioheart_mae[,,'Proteomics'],
                 colDataCols = c("age", "smurfs", "cacs", "cacs_pct", "gensini"))


prot_df = data.frame(prot_exp) %>%
  filter(!grepl("Repeat", colname)) %>%
  filter(primary != "79") %>%
  filter(primary != "Pool") %>%
  filter(primary != "Pool") %>%
  pivot_wider(id_cols = c(primary, age, smurfs, cacs, cacs_pct, gensini), 
              names_from = rowname, values_from = value) %>%
  transform(cacs = as.numeric(cacs), cacs_pct = as.numeric(cacs_pct), gensini = as.numeric(gensini), primary = as.numeric(primary))

score_df %>%
  select(ID, consensus_class) %>%
  inner_join(prot_df, by=c('ID'='primary')) %>%
  filter(consensus_class %in% c("resilient", "reference")) %>%
  pivot_longer(cols = -c(ID, consensus_class, age, smurfs, cacs, cacs_pct, gensini), names_to = "proteins", values_to = "value") %>%
  ggplot() +
  aes(x = proteins, y = value, colour = consensus_class) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```

```{r}

prot_patient_df = score_df %>%
  select(ID, consensus_class) %>%
  inner_join(prot_df, by=c('ID'='primary'))


lm_df = prot_patient_df %>%
  select(-c(ID, consensus_class, age, smurfs, cacs, cacs_pct, gensini)) %>%
  t()

design = model.matrix(~ consensus_class, prot_patient_df)

rownames(lm_df)
dim(lm_df)
dim(design)
fit = lmFit(lm_df, design)
fit = eBayes(fit)
topTable(fit)

CM = makeContrasts(consensus_classresilient = consensus_classresilient - consensus_classsusceptible, levels = design)
constrast_fit = contrasts.fit(fit, contrast = CM)
constrast_fit = eBayes(constrast_fit)
topTable(constrast_fit)

table(filtered_graph_df$consensus_class)

```



```{r}
# Differential Expression
# https://link.springer.com/article/10.1007/s11306-015-0823-6
# control for age, gender (Emwas et al. 2014), diet (Heinzmann et al. 2010), physical activity

```


```{r}
dat_FRS
library(CVrisk)


dat_FRS %>%
  select(age, gender, bmi, cvhx_dm, curr_smok, sbp, cacs, HDL)


colnames(dat_FRS)

lm()
```

```{r}

ensemble_residuals = function(y, df, ...) {
  for(model in list(...)){
    predictions = predict(model, df)
    
  }
}

predict(elastic_model, X_test)

resilience_residuals = residuals(cacs_FRS_model)

test = lm(age ~ sbp, dat_FRS)

predict(test, dat_FRS)

```



## Junk Code

### Non-Linear SVM-RFE

-   Uses a non-linear kernel
-   Recursive Feature Elimination, filters for greatest importance
-   Create predictive rules for data that cannot be classified by linear decision functions

<https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2451-4>

(1) A SVM model was fitted using the specified differential metabolites as independent variables and grouping information as dependent variables;
(2) Calculate the importance weight of each metabolite;
(3) If the number of remaining metabolites is less than the specified number of metabolites, end the screening and return the importance weight of each metabolite;
(4) Eliminate the metabolites with the lowest weight;
(5) Repeat (1)--(4)

<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7169426/> <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9222502/>
